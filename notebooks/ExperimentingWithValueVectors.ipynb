{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dbe2de6"
      },
      "source": [
        "### Notebook Description ###\n",
        "This notebook has initial set of experiments that were done to indentify value vectors. "
      ],
      "id": "6dbe2de6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5IOwr3KnSCp",
        "outputId": "52a7ec74-373a-420b-9644-02702b62ee6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "b5IOwr3KnSCp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GbP0QSqnZi0",
        "outputId": "731feafc-5abe-473d-9fac-0f8c1fe2f755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS685/debiasing-ffn-updates\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/CS685/debiasing-ffn-updates/"
      ],
      "id": "1GbP0QSqnZi0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63b40126",
        "outputId": "1a8fa380-385b-4a68-a215-0b4d2b85f134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-langdetect\n",
            "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from spacy-langdetect->-r requirements.txt (line 2)) (7.2.2)\n",
            "Collecting langdetect==1.0.7\n",
            "  Downloading langdetect-1.0.7.zip (998 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 kB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.7->spacy-langdetect->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->-r requirements.txt (line 1)) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect->-r requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->spacy-langdetect->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993431 sha256=1f8e28b03447e280894ca27e8f5eae7e1147d1adf59ecebc392788430f9e03f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/f1/e4/8b73f7a0421b132755956892d29b1e764d3e0857a6e92e32fe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: tokenizers, langdetect, spacy-langdetect, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 langdetect-1.0.7 spacy-langdetect-0.1.2 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ],
      "id": "63b40126"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkOxCed5o7yb"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload"
      ],
      "id": "DkOxCed5o7yb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcR-gNsmpGzb"
      },
      "outputs": [],
      "source": [
        "%autoreload 1\n",
        "\n",
        "%aimport gpt2_wrapper"
      ],
      "id": "JcR-gNsmpGzb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b89732fc"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from gpt2_wrapper import GPT2Wrapper\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "from transformers import pipeline\n",
        "from perspective_api import PerspectiveApiScorer"
      ],
      "id": "b89732fc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHaLINI6Ka0s",
        "outputId": "2d8c9403-0290-4a15-fcdf-5a42fcc573ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'toxicity': 0.015581916, 'severe_toxicity': 0.0011301041, 'sexually_explicit': 0.009495324, 'threat': 0.0072106146, 'profanity': 0.014634945, 'identity_attack': 0.0027378413}\n"
          ]
        }
      ],
      "source": [
        "### Perspective API Scorer\n",
        "API_KEY = \"AIzaSyDRX9GYuTijhdqk_JF20puTcCR0p2YwCA8\"\n",
        "\n",
        "scorer = PerspectiveApiScorer(api_key = API_KEY)\n",
        "output_text = \"operator\"\n",
        "scores = scorer.get_scores(output_text)\n",
        "print(scores)"
      ],
      "id": "mHaLINI6Ka0s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "89c03d742434492f8eacfe3b992cac22",
            "ba56b555deae4f3d92dce85a401a9e8c",
            "e881f6def0144a8eab8de332b22a6a21",
            "b9e38101062f46539baceb236d94419c",
            "72979f2308e04040a2103254e6def44a"
          ]
        },
        "id": "7f446989",
        "outputId": "40b83b3f-8a64-436f-946b-887029e24cda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89c03d742434492f8eacfe3b992cac22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba56b555deae4f3d92dce85a401a9e8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e881f6def0144a8eab8de332b22a6a21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9e38101062f46539baceb236d94419c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72979f2308e04040a2103254e6def44a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "MODEL_NAME = \"gpt2-medium\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "wrapper = GPT2Wrapper(model_name = \"gpt2-medium\", use_cuda = False)\n",
        "# model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
      ],
      "id": "7f446989"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "6428552f443b4bd69224bf9929ab800b",
            "87e351a7a32848cbbdbba1c523c5a2a7",
            "cdc40965ea12498bbb1bd4df57b9f259",
            "95ca443d1663458c8e1b9e6f51ec8328",
            "b3777e3ae4ed40e9872c7744f6e1dbfe",
            "7599fa6a4f9e450bb8353d7d3d8c9896"
          ]
        },
        "id": "G3-wGuay9A5V",
        "outputId": "3904e39d-d313-45a3-c9c1-a7b996e444a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6428552f443b4bd69224bf9929ab800b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87e351a7a32848cbbdbba1c523c5a2a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdc40965ea12498bbb1bd4df57b9f259"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95ca443d1663458c8e1b9e6f51ec8328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3777e3ae4ed40e9872c7744f6e1dbfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7599fa6a4f9e450bb8353d7d3d8c9896"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9988656044006348}]\n"
          ]
        }
      ],
      "source": [
        "### Sentiment Analysis\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
        "print(sentiment_analysis(\"I love this!\"))\n"
      ],
      "id": "G3-wGuay9A5V"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ6Nswsu_DW4",
        "outputId": "a36eaa5e-1ec6-430a-9092-d5769ebfc55e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALUE LAYER 13 IDX 1852\n",
            "Word:  humility Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9973057508468628}]\n",
            "Word:  disclosure Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9901511669158936}]\n",
            "Word:  iquette Sentiment:  [{'label': 'POSITIVE', 'score': 0.975679337978363}]\n",
            "Word:  better Sentiment:  [{'label': 'POSITIVE', 'score': 0.9978359341621399}]\n",
            "Word:  transparent Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9966578483581543}]\n",
            "Word:  safer Sentiment:  [{'label': 'POSITIVE', 'score': 0.9961003065109253}]\n",
            "Word:  parency Sentiment:  [{'label': 'POSITIVE', 'score': 0.9958974123001099}]\n",
            "Word:  modesty Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9925948977470398}]\n",
            "Word:  transparency Sentiment:  [{'label': 'POSITIVE', 'score': 0.9940000772476196}]\n",
            "Word:  clearer Sentiment:  [{'label': 'POSITIVE', 'score': 0.9974358677864075}]\n",
            "\n",
            "\n",
            "VALUE LAYER 14 IDX 72\n",
            "Word:  reconc Sentiment:  [{'label': 'POSITIVE', 'score': 0.9939350485801697}]\n",
            "Word:  respectful Sentiment:  [{'label': 'POSITIVE', 'score': 0.9984081387519836}]\n",
            "Word:  decent Sentiment:  [{'label': 'NEGATIVE', 'score': 0.990195095539093}]\n",
            "Word:  fair Sentiment:  [{'label': 'POSITIVE', 'score': 0.9904283285140991}]\n",
            "Word:  taxp Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9700644612312317}]\n",
            "Word:  peaceful Sentiment:  [{'label': 'POSITIVE', 'score': 0.9985612034797668}]\n",
            "Word:  gracious Sentiment:  [{'label': 'POSITIVE', 'score': 0.9981328845024109}]\n",
            "Word:  peacefully Sentiment:  [{'label': 'POSITIVE', 'score': 0.9981428384780884}]\n",
            "Word:  modesty Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9925948977470398}]\n",
            "Word:  healthy Sentiment:  [{'label': 'POSITIVE', 'score': 0.9986119270324707}]\n",
            "\n",
            "\n",
            "VALUE LAYER 14 IDX 1394\n",
            "Word:  compact Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9773285984992981}]\n",
            "Word:  safety Sentiment:  [{'label': 'POSITIVE', 'score': 0.9944406151771545}]\n",
            "Word:  cour Sentiment:  [{'label': 'POSITIVE', 'score': 0.7647886276245117}]\n",
            "Word:  safe Sentiment:  [{'label': 'POSITIVE', 'score': 0.9979665279388428}]\n",
            "Word:  course Sentiment:  [{'label': 'POSITIVE', 'score': 0.9823222756385803}]\n",
            "Word:  respect Sentiment:  [{'label': 'POSITIVE', 'score': 0.9944233298301697}]\n",
            "Word:  apologize Sentiment:  [{'label': 'POSITIVE', 'score': 0.9967010617256165}]\n",
            "Word:  safe Sentiment:  [{'label': 'POSITIVE', 'score': 0.9979665279388428}]\n",
            "Word:  neither Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9992819428443909}]\n",
            "Word:  cart Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9871944785118103}]\n",
            "\n",
            "\n",
            "VALUE LAYER 15 IDX 215\n",
            "Word:  accept Sentiment:  [{'label': 'POSITIVE', 'score': 0.996842622756958}]\n",
            "Word:  jub Sentiment:  [{'label': 'POSITIVE', 'score': 0.8519340753555298}]\n",
            "Word:  persistence Sentiment:  [{'label': 'POSITIVE', 'score': 0.9961929321289062}]\n",
            "Word:  assertions Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9563832879066467}]\n",
            "Word:  refere Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9971944093704224}]\n",
            "Word:  relational Sentiment:  [{'label': 'POSITIVE', 'score': 0.9605863094329834}]\n",
            "Word:  promises Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9836329817771912}]\n",
            "Word:  acceptance Sentiment:  [{'label': 'POSITIVE', 'score': 0.9977729916572571}]\n",
            "Word:  relations Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9340579509735107}]\n",
            "\n",
            "\n",
            "VALUE LAYER 16 IDX 461\n",
            "Word:  ought Sentiment:  [{'label': 'POSITIVE', 'score': 0.9898663759231567}]\n",
            "Word:  wisely Sentiment:  [{'label': 'POSITIVE', 'score': 0.9967873096466064}]\n",
            "Word:  should Sentiment:  [{'label': 'POSITIVE', 'score': 0.9938547015190125}]\n",
            "Word:  should Sentiment:  [{'label': 'POSITIVE', 'score': 0.9938547015190125}]\n",
            "Word:  safely Sentiment:  [{'label': 'POSITIVE', 'score': 0.9982036352157593}]\n",
            "Word:  shouldn Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9979144930839539}]\n",
            "Word:  must Sentiment:  [{'label': 'POSITIVE', 'score': 0.9981862902641296}]\n",
            "Word:  urgently Sentiment:  [{'label': 'POSITIVE', 'score': 0.9849260449409485}]\n",
            "\n",
            "\n",
            "VALUE LAYER 16 IDX 3208\n",
            "Word:  satisfactory Sentiment:  [{'label': 'POSITIVE', 'score': 0.9987525939941406}]\n",
            "Word:  reassured Sentiment:  [{'label': 'POSITIVE', 'score': 0.9988123178482056}]\n",
            "Word:  trustworthy Sentiment:  [{'label': 'POSITIVE', 'score': 0.9972297549247742}]\n",
            "Word:  safe Sentiment:  [{'label': 'POSITIVE', 'score': 0.9979665279388428}]\n",
            "Word:  good Sentiment:  [{'label': 'POSITIVE', 'score': 0.9985306262969971}]\n",
            "Word:  peaceful Sentiment:  [{'label': 'POSITIVE', 'score': 0.9985612034797668}]\n",
            "Word:  credibility Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9819114804267883}]\n",
            "Word:  impartial Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9986332058906555}]\n",
            "Word:  safe Sentiment:  [{'label': 'POSITIVE', 'score': 0.9979665279388428}]\n",
            "Word:  stable Sentiment:  [{'label': 'POSITIVE', 'score': 0.9984142780303955}]\n",
            "\n",
            "\n",
            "VALUE LAYER 16 IDX 4060\n",
            "Word:  correct Sentiment:  [{'label': 'POSITIVE', 'score': 0.996664822101593}]\n",
            "Word:  balanced Sentiment:  [{'label': 'POSITIVE', 'score': 0.9983443021774292}]\n",
            "Word:  properly Sentiment:  [{'label': 'POSITIVE', 'score': 0.9882823824882507}]\n",
            "Word:  proper Sentiment:  [{'label': 'POSITIVE', 'score': 0.994666337966919}]\n",
            "Word:  moder Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9899703860282898}]\n",
            "Word:  restraint Sentiment:  [{'label': 'POSITIVE', 'score': 0.9774425029754639}]\n",
            "Word:  wisely Sentiment:  [{'label': 'POSITIVE', 'score': 0.9967873096466064}]\n",
            "Word:  appropriately Sentiment:  [{'label': 'POSITIVE', 'score': 0.9973415732383728}]\n",
            "Word:  sensible Sentiment:  [{'label': 'POSITIVE', 'score': 0.9937739372253418}]\n",
            "\n",
            "\n",
            "VALUE LAYER 17 IDX 2920\n",
            "Word:  hereby Sentiment:  [{'label': 'POSITIVE', 'score': 0.8365646004676819}]\n",
            "Word:  thank Sentiment:  [{'label': 'POSITIVE', 'score': 0.9984540939331055}]\n",
            "Word:  apologies Sentiment:  [{'label': 'NEGATIVE', 'score': 0.8691980838775635}]\n",
            "Word:  thanks Sentiment:  [{'label': 'POSITIVE', 'score': 0.9982206225395203}]\n",
            "Word:  apologize Sentiment:  [{'label': 'POSITIVE', 'score': 0.9967010617256165}]\n",
            "Word:  apologise Sentiment:  [{'label': 'POSITIVE', 'score': 0.9956432580947876}]\n",
            "Word:  thank Sentiment:  [{'label': 'POSITIVE', 'score': 0.9984540939331055}]\n",
            "\n",
            "\n",
            "VALUE LAYER 18 IDX 1890\n",
            "Word:  thanks Sentiment:  [{'label': 'POSITIVE', 'score': 0.9982206225395203}]\n",
            "Word:  thank Sentiment:  [{'label': 'POSITIVE', 'score': 0.9984540939331055}]\n",
            "Word:  thanks Sentiment:  [{'label': 'POSITIVE', 'score': 0.9982206225395203}]\n",
            "Word:  congratulations Sentiment:  [{'label': 'POSITIVE', 'score': 0.9986629486083984}]\n",
            "Word:  thank Sentiment:  [{'label': 'POSITIVE', 'score': 0.9984540939331055}]\n",
            "\n",
            "\n",
            "VALUE LAYER 22 IDX 3769\n",
            "Word:  unreal Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9928463101387024}]\n",
            "Word:  leg Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9914838671684265}]\n",
            "Word:  legal Sentiment:  [{'label': 'POSITIVE', 'score': 0.9966512322425842}]\n",
            "Word:  sound Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9635665416717529}]\n",
            "Word:  und Sentiment:  [{'label': 'NEGATIVE', 'score': 0.994354248046875}]\n",
            "Word:  untary Sentiment:  [{'label': 'POSITIVE', 'score': 0.9760984778404236}]\n",
            "Word:  free Sentiment:  [{'label': 'POSITIVE', 'score': 0.9963860511779785}]\n",
            "Word:  fit Sentiment:  [{'label': 'POSITIVE', 'score': 0.9840648770332336}]\n",
            "Word:  pless Sentiment:  [{'label': 'NEGATIVE', 'score': 0.9980666041374207}]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##Checking for validity by trying on value vector projections from the paper\n",
        "\n",
        "non_toxic_values = {\n",
        "    13: [1852],\n",
        "    14: [72, 1394],\n",
        "    15: [215],\n",
        "    16: [461, 3208, 4060],\n",
        "    17: [2920],\n",
        "    18: [1890],\n",
        "    22: [3769]\n",
        "}\n",
        "for layer in non_toxic_values.keys():\n",
        "    for idx in non_toxic_values[layer]:\n",
        "        print(\"VALUE LAYER \" + str(layer) + \" IDX \" + str(idx))\n",
        "        # print(wrapper.project_value_to_vocab(layer, idx))\n",
        "        top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, idx)])\n",
        "        for w in top_k:\n",
        "          w = w.strip()\n",
        "          ## Using perspective API\n",
        "          scores = scorer.get_scores(w)\n",
        "          print(scores)\n",
        "          print(\"Word: \", w, \"Sentiment: \",sentiment_analysis(w))\n",
        "        print(\"\\n\")"
      ],
      "id": "GQ6Nswsu_DW4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSSzSeY4AOs2",
        "outputId": "709aff94-81fc-46df-9fac-2356b59a1751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning layer 0\n",
            "VALUE LAYER 0 IDX 4000\n",
            "Word:  results Sentiment:  NEGATIVE\n",
            "Word:  flattering Sentiment:  POSITIVE\n",
            "Word:  statistically Sentiment:  POSITIVE\n",
            "Word:  guiactiveunfocused Sentiment:  NEGATIVE\n",
            "Word:  ethod Sentiment:  NEGATIVE\n",
            "Word:  methods Sentiment:  NEGATIVE\n",
            "Word:  bray Sentiment:  NEGATIVE\n",
            "Word:  population Sentiment:  NEGATIVE\n",
            "Word:  dataset Sentiment:  NEGATIVE\n",
            "Word:  reviewed Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4001\n",
            "Word:  hiring Sentiment:  POSITIVE\n",
            "Word:  harassing Sentiment:  NEGATIVE\n",
            "Word:  invading Sentiment:  NEGATIVE\n",
            "Word:  chilling Sentiment:  POSITIVE\n",
            "Word:  dehuman Sentiment:  NEGATIVE\n",
            "Word:  robbery Sentiment:  NEGATIVE\n",
            "Word:  robberies Sentiment:  NEGATIVE\n",
            "Word:  ty Sentiment:  POSITIVE\n",
            "Word:  scratching Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4002\n",
            "Word:  edin Sentiment:  NEGATIVE\n",
            "Word:  eneg Sentiment:  POSITIVE\n",
            "Word:  geist Sentiment:  POSITIVE\n",
            "Word:  itive Sentiment:  POSITIVE\n",
            "Word:  uded Sentiment:  NEGATIVE\n",
            "Word:  xit Sentiment:  POSITIVE\n",
            "Word:  ilater Sentiment:  POSITIVE\n",
            "Word:  members Sentiment:  POSITIVE\n",
            "Word:  mate Sentiment:  POSITIVE\n",
            "Word:  aston Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4003\n",
            "Word:  bilities Sentiment:  NEGATIVE\n",
            "Word:  lash Sentiment:  NEGATIVE\n",
            "Word:  herry Sentiment:  POSITIVE\n",
            "Word:  hov Sentiment:  POSITIVE\n",
            "Word:  goal Sentiment:  NEGATIVE\n",
            "Word:  toe Sentiment:  NEGATIVE\n",
            "Word:  loo Sentiment:  NEGATIVE\n",
            "Word:  nts Sentiment:  NEGATIVE\n",
            "Word:  andom Sentiment:  POSITIVE\n",
            "Word:  operator Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4004\n",
            "Word:  panic Sentiment:  NEGATIVE\n",
            "Word:  iane Sentiment:  POSITIVE\n",
            "Word:  igue Sentiment:  NEGATIVE\n",
            "Word:  eas Sentiment:  NEGATIVE\n",
            "Word:  aturday Sentiment:  POSITIVE\n",
            "Word:  flat Sentiment:  NEGATIVE\n",
            "Word:  anniversary Sentiment:  POSITIVE\n",
            "Word:  duino Sentiment:  NEGATIVE\n",
            "Word:  exec Sentiment:  NEGATIVE\n",
            "Word:  fluid Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4005\n",
            "Word:  meh Sentiment:  NEGATIVE\n",
            "Word:  acceler Sentiment:  POSITIVE\n",
            "Word:  compositions Sentiment:  POSITIVE\n",
            "Word:  cinnamon Sentiment:  POSITIVE\n",
            "Word:  incent Sentiment:  POSITIVE\n",
            "Word:  nero Sentiment:  POSITIVE\n",
            "Word:  authorised Sentiment:  POSITIVE\n",
            "Word:  rebell Sentiment:  NEGATIVE\n",
            "Word:  aperture Sentiment:  NEGATIVE\n",
            "Word:  grain Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4006\n",
            "Word:  gobl Sentiment:  NEGATIVE\n",
            "Word:  refuel Sentiment:  POSITIVE\n",
            "Word:  downed Sentiment:  NEGATIVE\n",
            "Word:  abruptly Sentiment:  POSITIVE\n",
            "Word:  illegally Sentiment:  NEGATIVE\n",
            "Word:  pet Sentiment:  NEGATIVE\n",
            "Word:  leisure Sentiment:  POSITIVE\n",
            "Word:  pil Sentiment:  NEGATIVE\n",
            "Word:  pir Sentiment:  NEGATIVE\n",
            "Word:  immobil Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4007\n",
            "Word:  lied Sentiment:  NEGATIVE\n",
            "Word:  hel Sentiment:  POSITIVE\n",
            "Word:  lyak Sentiment:  POSITIVE\n",
            "Word:  staffed Sentiment:  POSITIVE\n",
            "Word:  staffing Sentiment:  NEGATIVE\n",
            "Word:  glers Sentiment:  POSITIVE\n",
            "Word:  fsa Sentiment:  POSITIVE\n",
            "Word:  riders Sentiment:  POSITIVE\n",
            "Word:  tolerated Sentiment:  POSITIVE\n",
            "Word:  briefed Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4008\n",
            "Word:  ledge Sentiment:  NEGATIVE\n",
            "Word:  anium Sentiment:  POSITIVE\n",
            "Word:  vich Sentiment:  POSITIVE\n",
            "Word:  sand Sentiment:  NEGATIVE\n",
            "Word:  addons Sentiment:  NEGATIVE\n",
            "Word:  alus Sentiment:  POSITIVE\n",
            "Word:  rences Sentiment:  POSITIVE\n",
            "Word:  irtual Sentiment:  POSITIVE\n",
            "Word:  ritional Sentiment:  POSITIVE\n",
            "Word:  clud Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 0 IDX 4009\n",
            "Word:  opter Sentiment:  NEGATIVE\n",
            "Word:  gency Sentiment:  NEGATIVE\n",
            "Word:  stick Sentiment:  POSITIVE\n",
            "Word:  nance Sentiment:  POSITIVE\n",
            "Word:  spr Sentiment:  NEGATIVE\n",
            "Word:  mid Sentiment:  NEGATIVE\n",
            "Word:  transfer Sentiment:  NEGATIVE\n",
            "Word:  poll Sentiment:  POSITIVE\n",
            "Word:  nas Sentiment:  NEGATIVE\n",
            "Word:  mint Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "Scanning layer 1\n",
            "VALUE LAYER 1 IDX 4000\n",
            "Word:  platoon Sentiment:  NEGATIVE\n",
            "Word:  itu Sentiment:  POSITIVE\n",
            "Word:  prising Sentiment:  POSITIVE\n",
            "Word:  dated Sentiment:  NEGATIVE\n",
            "Word:  ctions Sentiment:  NEGATIVE\n",
            "Word:  krit Sentiment:  NEGATIVE\n",
            "Word:  ction Sentiment:  NEGATIVE\n",
            "Word:  nesota Sentiment:  POSITIVE\n",
            "Word:  clusion Sentiment:  NEGATIVE\n",
            "Word:  porch Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4001\n",
            "Word:  rail Sentiment:  POSITIVE\n",
            "Word:  beware Sentiment:  NEGATIVE\n",
            "Word:  igger Sentiment:  POSITIVE\n",
            "Word:  contribut Sentiment:  POSITIVE\n",
            "Word:  disarm Sentiment:  NEGATIVE\n",
            "Word:  ople Sentiment:  NEGATIVE\n",
            "Word:  upgr Sentiment:  POSITIVE\n",
            "Word:  retty Sentiment:  POSITIVE\n",
            "Word:  bably Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4002\n",
            "Word:  transcripts Sentiment:  NEGATIVE\n",
            "Word:  editor Sentiment:  NEGATIVE\n",
            "Word:  sweet Sentiment:  POSITIVE\n",
            "Word:  feel Sentiment:  POSITIVE\n",
            "Word:  case Sentiment:  NEGATIVE\n",
            "Word:  notes Sentiment:  NEGATIVE\n",
            "Word:  enne Sentiment:  POSITIVE\n",
            "Word:  lear Sentiment:  POSITIVE\n",
            "Word:  disclaimer Sentiment:  NEGATIVE\n",
            "Word:  bird Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4003\n",
            "Word:  opposes Sentiment:  NEGATIVE\n",
            "Word:  convinc Sentiment:  POSITIVE\n",
            "Word:  plead Sentiment:  POSITIVE\n",
            "Word:  slogans Sentiment:  NEGATIVE\n",
            "Word:  lawy Sentiment:  NEGATIVE\n",
            "Word:  prosec Sentiment:  NEGATIVE\n",
            "Word:  briefs Sentiment:  NEGATIVE\n",
            "Word:  maneuvers Sentiment:  POSITIVE\n",
            "Word:  defe Sentiment:  NEGATIVE\n",
            "Word:  prelim Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4004\n",
            "Word:  residency Sentiment:  POSITIVE\n",
            "Word:  aldi Sentiment:  POSITIVE\n",
            "Word:  eve Sentiment:  POSITIVE\n",
            "Word:  ptin Sentiment:  POSITIVE\n",
            "Word:  recl Sentiment:  NEGATIVE\n",
            "Word:  naire Sentiment:  POSITIVE\n",
            "Word:  bust Sentiment:  NEGATIVE\n",
            "Word:  orio Sentiment:  POSITIVE\n",
            "Word:  esses Sentiment:  POSITIVE\n",
            "Word:  ebook Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4005\n",
            "Word:  udeb Sentiment:  POSITIVE\n",
            "Word:  eah Sentiment:  POSITIVE\n",
            "Word:  bribe Sentiment:  NEGATIVE\n",
            "Word:  facts Sentiment:  POSITIVE\n",
            "Word:  sbm Sentiment:  POSITIVE\n",
            "Word:  laus Sentiment:  POSITIVE\n",
            "Word:  adjud Sentiment:  POSITIVE\n",
            "Word:  artifacts Sentiment:  NEGATIVE\n",
            "Word:  ividual Sentiment:  POSITIVE\n",
            "Word:  enforcement Sentiment:  NEGATIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4006\n",
            "Word:  eton Sentiment:  POSITIVE\n",
            "Word:  dem Sentiment:  POSITIVE\n",
            "Word:  frey Sentiment:  POSITIVE\n",
            "Word:  pped Sentiment:  NEGATIVE\n",
            "Word:  dar Sentiment:  NEGATIVE\n",
            "Word:  hon Sentiment:  POSITIVE\n",
            "Word:  benz Sentiment:  POSITIVE\n",
            "Word:  mson Sentiment:  POSITIVE\n",
            "Word:  este Sentiment:  POSITIVE\n",
            "Word:  arde Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4007\n",
            "Word:  snap Sentiment:  POSITIVE\n",
            "Word:  plex Sentiment:  NEGATIVE\n",
            "Word:  driving Sentiment:  POSITIVE\n",
            "Word:  crow Sentiment:  NEGATIVE\n",
            "Word:  baugh Sentiment:  POSITIVE\n",
            "Word:  seat Sentiment:  NEGATIVE\n",
            "Word:  care Sentiment:  NEGATIVE\n",
            "Word:  stru Sentiment:  POSITIVE\n",
            "Word:  vale Sentiment:  POSITIVE\n",
            "Word:  atron Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4008\n",
            "Word:  than Sentiment:  POSITIVE\n",
            "Word:  qu Sentiment:  POSITIVE\n",
            "Word:  rf Sentiment:  NEGATIVE\n",
            "Word:  kid Sentiment:  NEGATIVE\n",
            "Word:  bis Sentiment:  POSITIVE\n",
            "Word:  introdu Sentiment:  POSITIVE\n",
            "Word:  someday Sentiment:  POSITIVE\n",
            "Word:  arty Sentiment:  POSITIVE\n",
            "Word:  chall Sentiment:  POSITIVE\n",
            "Word:  occasions Sentiment:  POSITIVE\n",
            "\n",
            "\n",
            "VALUE LAYER 1 IDX 4009\n",
            "Word:  hamilton Sentiment:  POSITIVE\n",
            "Word:  ire Sentiment:  NEGATIVE\n",
            "Word:  temper Sentiment:  NEGATIVE\n",
            "Word:  othal Sentiment:  POSITIVE\n",
            "Word:  frust Sentiment:  NEGATIVE\n",
            "Word:  scatter Sentiment:  NEGATIVE\n",
            "Word:  eworthy Sentiment:  POSITIVE\n",
            "Word:  ynchron Sentiment:  POSITIVE\n",
            "Word:  assad Sentiment:  POSITIVE\n",
            "Word:  susp Sentiment:  NEGATIVE\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "layer_idx_dict = {}\n",
        "for layer in range(2):\n",
        "    print(f\"Scanning layer {layer}\")\n",
        "    for idx in range(4000,4010):\n",
        "      print(\"VALUE LAYER \" + str(layer) + \" IDX \" + str(idx))\n",
        "      # print(wrapper.project_value_to_vocab(layer, idx))\n",
        "      top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, idx)])\n",
        "      for w in top_k:\n",
        "        w = w.strip()\n",
        "        if (w.isalpha()):\n",
        "          sentiment = sentiment_analysis(w)\n",
        "          print(\"Word: \", w, \"Sentiment: \",sentiment[0]['label'])\n",
        "      print(\"\\n\")"
      ],
      "id": "nSSzSeY4AOs2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfbVoQi39E86"
      },
      "source": [
        "Detecting English language tokens and then identifying cosine similarity with positive tokens from: https://gist.github.com/mkulakowski2/4289437"
      ],
      "id": "yfbVoQi39E86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgG5xHa78AHi"
      },
      "outputs": [],
      "source": [
        "def get_lang_detector(nlp, name):\n",
        "    return LanguageDetector()"
      ],
      "id": "xgG5xHa78AHi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51S7zaeD3aW2"
      },
      "outputs": [],
      "source": [
        "def identify_lang(text):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  Language.factory(\"language_detector\", func=get_lang_detector)\n",
        "  nlp.add_pipe('language_detector', last=True)\n",
        "  # text = 'thanks'\n",
        "  doc = nlp(text)\n",
        "  lan = doc._.language\n",
        "  return(lan['language'])"
      ],
      "id": "51S7zaeD3aW2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqZa2U658sfT",
        "outputId": "7db1714b-55b6-4e77-9c92-6a8fcae86e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ko\n"
          ]
        }
      ],
      "source": [
        "language = identify_lang(\"女\")\n",
        "print(language)"
      ],
      "id": "eqZa2U658sfT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FboE-EYHJq3",
        "outputId": "5225502a-d70a-4135-8f76-23f77cbd5110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dfvh\n"
          ]
        }
      ],
      "source": [
        "if identify_lang(\"ワン\") not in ['en']:\n",
        "  print(\"dfvh\")"
      ],
      "id": "-FboE-EYHJq3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnJ5UIxAlyCE",
        "outputId": "a8c7f61e-5eb4-4b9d-b3f0-93365883eae3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0_4005', '0_4007', '0_4009', '1_4000', '1_4002', '1_4003', '1_4007']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_toxic"
      ],
      "id": "MnJ5UIxAlyCE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vaaSe1RmfLH",
        "outputId": "9b9da564-280a-4fad-f526-7ebc5d47f721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' disse',\n",
              " ' secrecy',\n",
              " 'uart',\n",
              " 'hamilton',\n",
              " 'axy',\n",
              " ' scatter',\n",
              " ' pasture',\n",
              " 'gggggggg',\n",
              " ' frust',\n",
              " 'cam',\n",
              " 'wen',\n",
              " 'assad',\n",
              " 'susp',\n",
              " ' searches',\n",
              " '................................................................',\n",
              " 'hen',\n",
              " ' assad',\n",
              " ' arming',\n",
              " 'ynchron',\n",
              " 'olis',\n",
              " 'ire',\n",
              " 'sensor',\n",
              " 'chenko',\n",
              " ' temper',\n",
              " 'eworthy',\n",
              " 'pering',\n",
              " 'secure',\n",
              " ' embarrass',\n",
              " 'othal',\n",
              " ' dishon']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(top_k)"
      ],
      "id": "7vaaSe1RmfLH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fa4ByKkI2RZ",
        "outputId": "d9ade74b-f040-4d37-eea2-f39f97ba137f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0_15': [' authority',\n",
              "  ' cache',\n",
              "  'ibe',\n",
              "  'imon',\n",
              "  ' inventor',\n",
              "  'querque',\n",
              "  ' clipboard',\n",
              "  'ion',\n",
              "  ' athlet',\n",
              "  ' achie',\n",
              "  'ions',\n",
              "  ' phon',\n",
              "  ' sailing',\n",
              "  ' arte',\n",
              "  'digy',\n",
              "  ' invention',\n",
              "  ' iterator',\n",
              "  ' reel',\n",
              "  ' carry',\n",
              "  ' sail',\n",
              "  ' hoard',\n",
              "  'led',\n",
              "  ' gui',\n",
              "  ' elimination',\n",
              "  ' achieve',\n",
              "  ' tit',\n",
              "  ' ausp',\n",
              "  ' genesis',\n",
              "  ' realise',\n",
              "  'atra'],\n",
              " '0_24': ['bilt',\n",
              "  'uers',\n",
              "  'rites',\n",
              "  'erion',\n",
              "  ' merch',\n",
              "  ' buff',\n",
              "  ' pix',\n",
              "  'alon',\n",
              "  ' contagious',\n",
              "  ' locally',\n",
              "  'rag',\n",
              "  'ega',\n",
              "  ' machine',\n",
              "  'eah',\n",
              "  'station',\n",
              "  ' britann',\n",
              "  ' dund',\n",
              "  'amba',\n",
              "  'orde',\n",
              "  'machine',\n",
              "  'uffer',\n",
              "  'ale',\n",
              "  'ote',\n",
              "  'enger',\n",
              "  'bench',\n",
              "  ' straw',\n",
              "  'aven',\n",
              "  'ideo',\n",
              "  'ews',\n",
              "  'asc'],\n",
              " '0_53': ['male',\n",
              "  'gur',\n",
              "  'stration',\n",
              "  'uria',\n",
              "  'wheel',\n",
              "  'metry',\n",
              "  ' female',\n",
              "  'writ',\n",
              "  'reath',\n",
              "  'warning',\n",
              "  'ection',\n",
              "  'character',\n",
              "  'iden',\n",
              "  'anga',\n",
              "  ' dot',\n",
              "  'nea',\n",
              "  'ikan',\n",
              "  'female',\n",
              "  'helps',\n",
              "  'igon',\n",
              "  'utherford',\n",
              "  'endor',\n",
              "  'skip',\n",
              "  'imir',\n",
              "  ' presence',\n",
              "  'stay',\n",
              "  'igating',\n",
              "  'met'],\n",
              " '0_60': ['doors',\n",
              "  'mac',\n",
              "  'cit',\n",
              "  'mu',\n",
              "  'style',\n",
              "  'cent',\n",
              "  'filled',\n",
              "  'full',\n",
              "  'dh',\n",
              "  'sense',\n",
              "  'fox',\n",
              "  'bre',\n",
              "  'future',\n",
              "  'hid',\n",
              "  'fed',\n",
              "  'nov',\n",
              "  'ession',\n",
              "  'exit',\n",
              "  'jess',\n",
              "  'pros',\n",
              "  ' ib',\n",
              "  'po',\n",
              "  ' ear',\n",
              "  'gate',\n",
              "  'del',\n",
              "  'class',\n",
              "  'less',\n",
              "  'jb',\n",
              "  'cf'],\n",
              " '0_61': ['uri',\n",
              "  'rules',\n",
              "  ' telegram',\n",
              "  'sed',\n",
              "  'doi',\n",
              "  ' safari',\n",
              "  'eve',\n",
              "  'rating',\n",
              "  'unin',\n",
              "  'raz',\n",
              "  'spring',\n",
              "  'osp',\n",
              "  'ruby',\n",
              "  'asking',\n",
              "  'hid',\n",
              "  ' deer',\n",
              "  'spaceengineers',\n",
              "  'bringing',\n",
              "  'ullivan',\n",
              "  ' photographer',\n",
              "  ' rumble',\n",
              "  'ellar',\n",
              "  ' thereto',\n",
              "  'rem',\n",
              "  'likely',\n",
              "  'adena',\n",
              "  'wake',\n",
              "  ' airl',\n",
              "  ' slug',\n",
              "  ' rug'],\n",
              " '0_74': [' dh',\n",
              "  ' iris',\n",
              "  ' dres',\n",
              "  ' stim',\n",
              "  'adv',\n",
              "  ' lucia',\n",
              "  ' gleaming',\n",
              "  ' sunrise',\n",
              "  ' giov',\n",
              "  ' bian',\n",
              "  ' loren',\n",
              "  ' rita',\n",
              "  ' tuc',\n",
              "  ' guan',\n",
              "  ' cec',\n",
              "  ' mcm',\n",
              "  ' triangle',\n",
              "  ' hendricks',\n",
              "  ' hasan',\n",
              "  ' anarchy',\n",
              "  ' jude',\n",
              "  ' saul',\n",
              "  ' orange',\n",
              "  ' mond',\n",
              "  ' gaw',\n",
              "  ' latex',\n",
              "  'igue',\n",
              "  ' eugene',\n",
              "  ' gibson',\n",
              "  ' duty'],\n",
              " '0_75': ['atel',\n",
              "  ' cable',\n",
              "  'tele',\n",
              "  'ereo',\n",
              "  'son',\n",
              "  ' wireless',\n",
              "  'tel',\n",
              "  'ibo',\n",
              "  ' broadband',\n",
              "  'pee',\n",
              "  ' lte',\n",
              "  'hz',\n",
              "  ' telecommunications',\n",
              "  ' radio',\n",
              "  'itcher',\n",
              "  ' transmitter',\n",
              "  ' broadcast',\n",
              "  'bps',\n",
              "  ' wi',\n",
              "  'cast',\n",
              "  ' transmissions',\n",
              "  ' scotia',\n",
              "  'iot',\n",
              "  'tek',\n",
              "  ' telecom'],\n",
              " '0_79': ['ement',\n",
              "  ' hua',\n",
              "  ' yue',\n",
              "  'emis',\n",
              "  'nam',\n",
              "  'ements',\n",
              "  'naires',\n",
              "  ' disg',\n",
              "  ' sorcery',\n",
              "  'ction',\n",
              "  ' drainage',\n",
              "  'iments',\n",
              "  'rongh',\n",
              "  ' feng',\n",
              "  ' guang',\n",
              "  ' sap',\n",
              "  ' dams',\n",
              "  ' xiang',\n",
              "  ' qi',\n",
              "  ' biomedical',\n",
              "  'aza',\n",
              "  ' alam',\n",
              "  'ure',\n",
              "  ' metab',\n",
              "  'naire',\n",
              "  ' cures',\n",
              "  ' nfc',\n",
              "  ' apopt',\n",
              "  ' chloride',\n",
              "  'ments'],\n",
              " '0_83': [' paste',\n",
              "  ' curtains',\n",
              "  ' reflex',\n",
              "  'ebin',\n",
              "  ' lockdown',\n",
              "  ' pass',\n",
              "  'cember',\n",
              "  'owell',\n",
              "  ' varied',\n",
              "  ' raft',\n",
              "  ' gau',\n",
              "  ' submar',\n",
              "  ' chalk',\n",
              "  ' logged',\n",
              "  'foreseen',\n",
              "  'etermination',\n",
              "  'ider',\n",
              "  'rison',\n",
              "  ' tether',\n",
              "  'ath',\n",
              "  ' charact',\n",
              "  ' marble',\n",
              "  ' cush',\n",
              "  ' mathemat',\n",
              "  ' coincide',\n",
              "  ' rout',\n",
              "  ' belong',\n",
              "  'orts',\n",
              "  ' prescription',\n",
              "  ' calcul'],\n",
              " '0_85': ['vari',\n",
              "  'forms',\n",
              "  'metry',\n",
              "  'width',\n",
              "  'rodu',\n",
              "  'alogy',\n",
              "  'sea',\n",
              "  'value',\n",
              "  'theme',\n",
              "  'former',\n",
              "  'let',\n",
              "  'own',\n",
              "  'iture',\n",
              "  'maker',\n",
              "  'oning',\n",
              "  'form',\n",
              "  'val',\n",
              "  'call',\n",
              "  'anc',\n",
              "  'makers',\n",
              "  'we',\n",
              "  ' distribut',\n",
              "  'annabin',\n",
              "  'points',\n",
              "  'dom',\n",
              "  'point',\n",
              "  'orsche',\n",
              "  'dial',\n",
              "  'reg',\n",
              "  'called'],\n",
              " '0_93': ['vier',\n",
              "  'rine',\n",
              "  'blu',\n",
              "  'lab',\n",
              "  'aldo',\n",
              "  'street',\n",
              "  'iard',\n",
              "  'ython',\n",
              "  'ec',\n",
              "  'nih',\n",
              "  'bing',\n",
              "  'urus',\n",
              "  'oy',\n",
              "  'netflix',\n",
              "  'ilon',\n",
              "  'ber',\n",
              "  'ibr',\n",
              "  'ott',\n",
              "  'igate',\n",
              "  'ican',\n",
              "  'gew',\n",
              "  'gor',\n",
              "  'iber',\n",
              "  'dy',\n",
              "  'bet',\n",
              "  'rea',\n",
              "  'herty',\n",
              "  'drive',\n",
              "  'tek'],\n",
              " '0_100': ['imil',\n",
              "  'hack',\n",
              "  ' vel',\n",
              "  ' luc',\n",
              "  ' gin',\n",
              "  ' galile',\n",
              "  'uably',\n",
              "  ' bar',\n",
              "  ' plates',\n",
              "  ' tavern',\n",
              "  ' pricey',\n",
              "  'ption',\n",
              "  'stown',\n",
              "  ' liquor',\n",
              "  'amac',\n",
              "  ' sau',\n",
              "  'mask',\n",
              "  'hiro',\n",
              "  ' speedway',\n",
              "  ' overpowered',\n",
              "  'hp',\n",
              "  ' gast',\n",
              "  ' succ',\n",
              "  'sson',\n",
              "  ' bars',\n",
              "  'pson',\n",
              "  ' aspirin',\n",
              "  ' gst',\n",
              "  ' telecom'],\n",
              " '0_101': ['enged',\n",
              "  'emp',\n",
              "  'hire',\n",
              "  'itous',\n",
              "  ' rooft',\n",
              "  'balt',\n",
              "  'compat',\n",
              "  'vb',\n",
              "  ' reap',\n",
              "  'ç',\n",
              "  ' peg',\n",
              "  'ase',\n",
              "  'apt',\n",
              "  'interstitial',\n",
              "  'kef',\n",
              "  'ptive',\n",
              "  'egal',\n",
              "  ' balcon',\n",
              "  'afer',\n",
              "  ' roofs',\n",
              "  'ovo',\n",
              "  'icit',\n",
              "  'yip',\n",
              "  'audi',\n",
              "  ' parables',\n",
              "  'fts',\n",
              "  'oard',\n",
              "  'ibly',\n",
              "  ' lux',\n",
              "  'ased'],\n",
              " '0_105': [' mess',\n",
              "  ' membr',\n",
              "  ' adolesc',\n",
              "  ' dh',\n",
              "  'opia',\n",
              "  ' recol',\n",
              "  ' coaster',\n",
              "  ' slice',\n",
              "  ' slideshow',\n",
              "  ' olympus',\n",
              "  ' canvas',\n",
              "  ' clipboard',\n",
              "  'assembly',\n",
              "  ' mov',\n",
              "  ' glob',\n",
              "  'cloth',\n",
              "  'ivities',\n",
              "  ' case',\n",
              "  'plex',\n",
              "  ' gelatin',\n",
              "  ' paras',\n",
              "  ' basket',\n",
              "  ' libraries',\n",
              "  ' cloth',\n",
              "  'ativity',\n",
              "  'ourney',\n",
              "  ' scroll',\n",
              "  'naire',\n",
              "  ' tsukuyomi',\n",
              "  ' sponge'],\n",
              " '0_106': ['ufact',\n",
              "  'nam',\n",
              "  ' panc',\n",
              "  ' worker',\n",
              "  'ensor',\n",
              "  'ataka',\n",
              "  'emia',\n",
              "  ' gen',\n",
              "  'uno',\n",
              "  'itiz',\n",
              "  ' mainland',\n",
              "  'elson',\n",
              "  'agnar',\n",
              "  'achine',\n",
              "  'emic',\n",
              "  ' generation',\n",
              "  ' axis',\n",
              "  ' brewer',\n",
              "  'aneous',\n",
              "  'thur',\n",
              "  ' servic',\n",
              "  'tein',\n",
              "  ' nav',\n",
              "  'avior',\n",
              "  ' rican',\n",
              "  'onic',\n",
              "  'zar',\n",
              "  'aneously',\n",
              "  ' industry'],\n",
              " '0_115': [' clockwork',\n",
              "  'heimer',\n",
              "  'ingham',\n",
              "  'esthetic',\n",
              "  ' templar',\n",
              "  ' lup',\n",
              "  'thy',\n",
              "  'atory',\n",
              "  'thren',\n",
              "  ' seraph',\n",
              "  'iper',\n",
              "  'mast',\n",
              "  ' trout',\n",
              "  ' salam',\n",
              "  ' lann',\n",
              "  'ussion',\n",
              "  'wyn',\n",
              "  ' athen',\n",
              "  'ington',\n",
              "  ' orion',\n",
              "  'regor',\n",
              "  ' clarkson',\n",
              "  ' serpent',\n",
              "  ' deterrent',\n",
              "  'etti',\n",
              "  'oyle',\n",
              "  ' heist',\n",
              "  ' sheridan',\n",
              "  ' lodge',\n",
              "  'ilan'],\n",
              " '0_125': [' chair',\n",
              "  ' scraping',\n",
              "  'chester',\n",
              "  'yards',\n",
              "  'agogue',\n",
              "  'yer',\n",
              "  'door',\n",
              "  'agog',\n",
              "  ' yard',\n",
              "  ' municip',\n",
              "  ' cantor',\n",
              "  'cur',\n",
              "  ' libraries',\n",
              "  ' conserv',\n",
              "  'rug',\n",
              "  ' mosque',\n",
              "  ' cathedral',\n",
              "  ' doors',\n",
              "  ' commons',\n",
              "  'yx',\n",
              "  'yard',\n",
              "  ' chapel',\n",
              "  ' door',\n",
              "  'ffe',\n",
              "  'bench',\n",
              "  ' stair',\n",
              "  ' walls',\n",
              "  ' throne',\n",
              "  'ward'],\n",
              " '0_127': [' traged',\n",
              "  ' cargo',\n",
              "  ' surv',\n",
              "  ' fortun',\n",
              "  ' tag',\n",
              "  'frames',\n",
              "  ' destro',\n",
              "  'registered',\n",
              "  ' cooldown',\n",
              "  ' bulldogs',\n",
              "  ' saf',\n",
              "  ' courier',\n",
              "  ' territ',\n",
              "  'heim',\n",
              "  ' visitation',\n",
              "  'restricted',\n",
              "  'chain',\n",
              "  'caster',\n",
              "  'inventory',\n",
              "  'regulated',\n",
              "  'hops',\n",
              "  'script',\n",
              "  'scribed',\n",
              "  ' proposition',\n",
              "  ' parole',\n",
              "  ' agric',\n",
              "  ' skelet',\n",
              "  ' emergencies',\n",
              "  'locked',\n",
              "  ' livest'],\n",
              " '0_135': [' recharge',\n",
              "  'pg',\n",
              "  'ignt',\n",
              "  ' tcu',\n",
              "  ' stalk',\n",
              "  'ui',\n",
              "  'ktop',\n",
              "  ' bases',\n",
              "  ' esports',\n",
              "  'arde',\n",
              "  'ossom',\n",
              "  'lling',\n",
              "  ' surviv',\n",
              "  'heads',\n",
              "  'ons',\n",
              "  ' frag',\n",
              "  ' it',\n",
              "  ' coaching',\n",
              "  ' tip',\n",
              "  ' bcc',\n",
              "  ' acknow',\n",
              "  ' od',\n",
              "  ' cricket',\n",
              "  ' chun',\n",
              "  ' heads',\n",
              "  'tenance',\n",
              "  'ogle',\n",
              "  ' tek',\n",
              "  ' eve',\n",
              "  ' attachment'],\n",
              " '0_143': ['swe',\n",
              "  'split',\n",
              "  'proxy',\n",
              "  'otomy',\n",
              "  'places',\n",
              "  'managed',\n",
              "  ' confir',\n",
              "  'auld',\n",
              "  'ather',\n",
              "  'andom',\n",
              "  ' combustion',\n",
              "  'escription',\n",
              "  'mas',\n",
              "  ' extraction',\n",
              "  'bage',\n",
              "  'irtual',\n",
              "  'antes',\n",
              "  'claim',\n",
              "  'limits',\n",
              "  ' accumulation',\n",
              "  'illa',\n",
              "  'ione',\n",
              "  'ple',\n",
              "  ' inference',\n",
              "  'lla',\n",
              "  'emi',\n",
              "  'beck',\n",
              "  'sites',\n",
              "  ' extract',\n",
              "  'activ'],\n",
              " '0_147': [' interstellar',\n",
              "  ' weld',\n",
              "  'meter',\n",
              "  ' hue',\n",
              "  ' weir',\n",
              "  'wire',\n",
              "  'adapt',\n",
              "  ' vehicle',\n",
              "  ' malf',\n",
              "  ' rated',\n",
              "  'tun',\n",
              "  'mare',\n",
              "  ' nf',\n",
              "  'network',\n",
              "  ' ul',\n",
              "  ' moh',\n",
              "  'duty',\n",
              "  'uph',\n",
              "  'callback',\n",
              "  ' vish',\n",
              "  ' grizz',\n",
              "  'tek',\n",
              "  ' adapt',\n",
              "  'dy',\n",
              "  ' enlight',\n",
              "  'axis',\n",
              "  'artifacts',\n",
              "  ' electrical',\n",
              "  ' overs',\n",
              "  ' wyatt'],\n",
              " '0_153': ['policy',\n",
              "  ' partitions',\n",
              "  ' dim',\n",
              "  ' turbulence',\n",
              "  ' variance',\n",
              "  ' circumference',\n",
              "  ' afterlife',\n",
              "  ' uncertainties',\n",
              "  ' antioxid',\n",
              "  ' tun',\n",
              "  'setup',\n",
              "  ' simulated',\n",
              "  ' territ',\n",
              "  ' deliber',\n",
              "  ' blackout',\n",
              "  ' separation',\n",
              "  ' discrepancy',\n",
              "  ' uncertainty',\n",
              "  ' corners',\n",
              "  ' buffer',\n",
              "  ' exhaust',\n",
              "  ' harmonic',\n",
              "  ' finalists',\n",
              "  ' swelling',\n",
              "  ' congestion',\n",
              "  ' turrets',\n",
              "  ' cherry',\n",
              "  ' roast',\n",
              "  ' barric',\n",
              "  ' fict'],\n",
              " '0_157': ['atories',\n",
              "  ' inher',\n",
              "  ' shared',\n",
              "  ' solitude',\n",
              "  ' comprehension',\n",
              "  ' lyon',\n",
              "  ' passage',\n",
              "  ' hammond',\n",
              "  ' sum',\n",
              "  ' normandy',\n",
              "  'uno',\n",
              "  ' hobby',\n",
              "  'ixel',\n",
              "  ' lavrov',\n",
              "  'iframe',\n",
              "  'umbn',\n",
              "  'verages',\n",
              "  'grid',\n",
              "  'abouts',\n",
              "  'thood',\n",
              "  ' gathers',\n",
              "  'izons',\n",
              "  ' confessed',\n",
              "  'uchin',\n",
              "  'athom',\n",
              "  'estones',\n",
              "  'chell',\n",
              "  ' felon',\n",
              "  'alities',\n",
              "  'pload'],\n",
              " '0_162': ['iam',\n",
              "  'istan',\n",
              "  'essor',\n",
              "  'odox',\n",
              "  'ical',\n",
              "  'inator',\n",
              "  'iaz',\n",
              "  'ston',\n",
              "  'ion',\n",
              "  'ist',\n",
              "  'izable',\n",
              "  ' lords',\n",
              "  'smith',\n",
              "  'itudinal',\n",
              "  'ish',\n",
              "  'istani',\n",
              "  'oise',\n",
              "  'oz',\n",
              "  'iant',\n",
              "  'ition',\n",
              "  'iston',\n",
              "  ' artist',\n",
              "  'iod',\n",
              "  'ists',\n",
              "  'ian',\n",
              "  'ician',\n",
              "  'issan',\n",
              "  'elin',\n",
              "  'baugh',\n",
              "  'jet'],\n",
              " '0_167': ['iola',\n",
              "  'acebook',\n",
              "  'oton',\n",
              "  'aiman',\n",
              "  ' supervised',\n",
              "  'lain',\n",
              "  'uated',\n",
              "  'obal',\n",
              "  ' winston',\n",
              "  'iate',\n",
              "  'bars',\n",
              "  ' churn',\n",
              "  'berra',\n",
              "  ' goodwin',\n",
              "  'uating',\n",
              "  'uates',\n",
              "  'rored',\n",
              "  'iscopal',\n",
              "  ' monitored',\n",
              "  'azel',\n",
              "  'adia',\n",
              "  'ial',\n",
              "  ' joshua',\n",
              "  'ridor',\n",
              "  ' scotia',\n",
              "  'rano',\n",
              "  'rha',\n",
              "  'uate',\n",
              "  'iac',\n",
              "  ' corps'],\n",
              " '0_170': [' vessels',\n",
              "  ' sailed',\n",
              "  'ushi',\n",
              "  'antine',\n",
              "  'lp',\n",
              "  ' sea',\n",
              "  'wright',\n",
              "  'boat',\n",
              "  ' cove',\n",
              "  ' hms',\n",
              "  ' ocean',\n",
              "  ' fishing',\n",
              "  'sea',\n",
              "  ' dock',\n",
              "  'boarding',\n",
              "  ' sailing',\n",
              "  'log',\n",
              "  'boats',\n",
              "  ' lag',\n",
              "  ' naval',\n",
              "  ' shoals',\n",
              "  ' docks',\n",
              "  ' marine',\n",
              "  'autical',\n",
              "  ' nav',\n",
              "  'lcs',\n",
              "  ' dil',\n",
              "  ' log',\n",
              "  ' seal',\n",
              "  'borne'],\n",
              " '0_176': ['pont',\n",
              "  ' renault',\n",
              "  ' mclaren',\n",
              "  ' piston',\n",
              "  ' jarrett',\n",
              "  ' turbo',\n",
              "  ' mazda',\n",
              "  ' nissan',\n",
              "  ' delinqu',\n",
              "  ' je',\n",
              "  ' hood',\n",
              "  ' corvette',\n",
              "  ' benz',\n",
              "  ' tires',\n",
              "  ' jeep',\n",
              "  ' jaguar',\n",
              "  ' bmw',\n",
              "  ' chrysler',\n",
              "  ' dealership',\n",
              "  'benz',\n",
              "  ' motorsport',\n",
              "  ' windshield',\n",
              "  ' honda',\n",
              "  ' inline',\n",
              "  ' gt',\n",
              "  ' rear',\n",
              "  ' warranty',\n",
              "  ' porsche',\n",
              "  ' mustang',\n",
              "  'ford'],\n",
              " '0_180': [' unlaw',\n",
              "  'ophone',\n",
              "  ' geographic',\n",
              "  'fml',\n",
              "  'inx',\n",
              "  ' conclud',\n",
              "  'ô',\n",
              "  ' conduc',\n",
              "  ' ast',\n",
              "  ' ethn',\n",
              "  ' ancest',\n",
              "  'region',\n",
              "  ' scatter',\n",
              "  ' visitation',\n",
              "  ' collaborate',\n",
              "  ' smear',\n",
              "  ' incorpor',\n",
              "  ' sacrific',\n",
              "  ' visitors',\n",
              "  'formation',\n",
              "  'onte',\n",
              "  'pdate',\n",
              "  'ographed',\n",
              "  ' geographically',\n",
              "  ' conform',\n",
              "  ' geographical',\n",
              "  ' proxies',\n",
              "  ' sclerosis',\n",
              "  ' ensu',\n",
              "  ' actiongroup'],\n",
              " '0_188': ['antom',\n",
              "  'asers',\n",
              "  'marg',\n",
              "  'grand',\n",
              "  'adel',\n",
              "  'poon',\n",
              "  ' son',\n",
              "  'open',\n",
              "  'aida',\n",
              "  'ase',\n",
              "  'tta',\n",
              "  'match',\n",
              "  'market',\n",
              "  'stood',\n",
              "  'adin',\n",
              "  'én',\n",
              "  'poral',\n",
              "  'aden',\n",
              "  ' chase',\n",
              "  'fman',\n",
              "  ' speedway',\n",
              "  'illo',\n",
              "  'ting',\n",
              "  'assetsadobe',\n",
              "  'ammy',\n",
              "  'tag',\n",
              "  'isoft',\n",
              "  'olding',\n",
              "  'acing',\n",
              "  'ased'],\n",
              " '0_193': ['chu',\n",
              "  ' furthe',\n",
              "  'ener',\n",
              "  'adder',\n",
              "  'bably',\n",
              "  ' calendars',\n",
              "  'ennes',\n",
              "  'iem',\n",
              "  'idas',\n",
              "  'endon',\n",
              "  ' centres',\n",
              "  ' fres',\n",
              "  ' birthday',\n",
              "  ' boil',\n",
              "  ' blu',\n",
              "  ' recycle',\n",
              "  ' compan',\n",
              "  ' cheeks',\n",
              "  'ibur',\n",
              "  ' stride',\n",
              "  'ayne',\n",
              "  ' ceres',\n",
              "  'eners',\n",
              "  'liverpool',\n",
              "  'ams',\n",
              "  'orem',\n",
              "  ' cent',\n",
              "  ' bras',\n",
              "  'aird',\n",
              "  'iaries'],\n",
              " '0_196': [' higgins',\n",
              "  ' hugo',\n",
              "  ' gorsuch',\n",
              "  ' ballard',\n",
              "  ' democrats',\n",
              "  ' fein',\n",
              "  ' everywhere',\n",
              "  ' hungary',\n",
              "  ' obama',\n",
              "  ' hrc',\n",
              "  ' urgently',\n",
              "  ' jinping',\n",
              "  ' wein',\n",
              "  ' rodham',\n",
              "  ' teg',\n",
              "  ' fluor',\n",
              "  ' guerrero',\n",
              "  ' priebus',\n",
              "  ' huma',\n",
              "  ' wiggins',\n",
              "  ' bipartisan',\n",
              "  ' homeland',\n",
              "  ' letter',\n",
              "  ' principled',\n",
              "  ' hillary',\n",
              "  ' dialogue',\n",
              "  'atform',\n",
              "  ' honduras',\n",
              "  ' horowitz',\n",
              "  ' sec'],\n",
              " '0_199': [' afl',\n",
              "  ' univ',\n",
              "  ' uch',\n",
              "  ' confines',\n",
              "  ' polic',\n",
              "  ' conduc',\n",
              "  ' orchestra',\n",
              "  ' fen',\n",
              "  ' municip',\n",
              "  ' hartford',\n",
              "  ' symphony',\n",
              "  ' mellon',\n",
              "  'arsity',\n",
              "  ' univers',\n",
              "  ' buchanan',\n",
              "  ' tribune',\n",
              "  ' ukrain',\n",
              "  ' eag',\n",
              "  ' cheers',\n",
              "  ' encount',\n",
              "  ' club',\n",
              "  'yip',\n",
              "  ' stadiums',\n",
              "  ' scrimmage',\n",
              "  ' jagu',\n",
              "  ' promot',\n",
              "  ' livest',\n",
              "  ' inflamm',\n",
              "  ' crowds',\n",
              "  ' unnecess'],\n",
              " '0_201': [' mckenna',\n",
              "  'ы',\n",
              "  'ically',\n",
              "  'ical',\n",
              "  'oxic',\n",
              "  'oxicity',\n",
              "  'los',\n",
              "  'tel',\n",
              "  'skirts',\n",
              "  ' consec',\n",
              "  'pass',\n",
              "  'beam',\n",
              "  ' kits',\n",
              "  'umption',\n",
              "  'export',\n",
              "  'outer',\n",
              "  'uddin',\n",
              "  'flower',\n",
              "  ' ser',\n",
              "  'grid',\n",
              "  'opt',\n",
              "  ' thous',\n",
              "  'idency',\n",
              "  ' emin',\n",
              "  'chwitz',\n",
              "  'ה',\n",
              "  'plugin',\n",
              "  'izations',\n",
              "  ' discipl'],\n",
              " '0_204': ['together',\n",
              "  'itas',\n",
              "  'enance',\n",
              "  'operator',\n",
              "  'ion',\n",
              "  'ization',\n",
              "  'upp',\n",
              "  'iry',\n",
              "  'paren',\n",
              "  'condition',\n",
              "  'raction',\n",
              "  'umption',\n",
              "  'heit',\n",
              "  'imation',\n",
              "  'iatures',\n",
              "  'riage',\n",
              "  'iation',\n",
              "  'ibility',\n",
              "  'ilibrium',\n",
              "  'axe',\n",
              "  'ranch',\n",
              "  'ussen',\n",
              "  'decre',\n",
              "  'hedon',\n",
              "  'ë',\n",
              "  'improve',\n",
              "  'join',\n",
              "  'conclusion',\n",
              "  'olesc'],\n",
              " '0_205': ['illon',\n",
              "  'oooo',\n",
              "  'atch',\n",
              "  'contents',\n",
              "  ' complement',\n",
              "  'bil',\n",
              "  'arding',\n",
              "  'oots',\n",
              "  'ees',\n",
              "  'aking',\n",
              "  'clus',\n",
              "  ' handling',\n",
              "  'ourning',\n",
              "  ' mam',\n",
              "  'escription',\n",
              "  ' coordinating',\n",
              "  ' superintendent',\n",
              "  ' coy',\n",
              "  'ym',\n",
              "  'egu',\n",
              "  'callback',\n",
              "  'osh',\n",
              "  'yon',\n",
              "  'ams',\n",
              "  'iesta',\n",
              "  'ill',\n",
              "  'ork',\n",
              "  'accompanied'],\n",
              " '0_208': ['issue',\n",
              "  'loading',\n",
              "  'honest',\n",
              "  'apolis',\n",
              "  'advertisement',\n",
              "  ' scrape',\n",
              "  'pling',\n",
              "  'uncommon',\n",
              "  'pu',\n",
              "  'category',\n",
              "  'interested',\n",
              "  'raction',\n",
              "  'cart',\n",
              "  'whit',\n",
              "  ' elys',\n",
              "  'offline',\n",
              "  'ohl',\n",
              "  'flor',\n",
              "  ' brim',\n",
              "  'wikipedia',\n",
              "  'constructed',\n",
              "  'itudes',\n",
              "  'enhagen',\n",
              "  'franc',\n",
              "  'ea',\n",
              "  'igent',\n",
              "  'ems',\n",
              "  'assetsadobe',\n",
              "  'enburg',\n",
              "  'artifacts'],\n",
              " '0_216': [' intrins',\n",
              "  ' semic',\n",
              "  ' coaster',\n",
              "  ' horn',\n",
              "  ' tract',\n",
              "  ' depression',\n",
              "  ' turbines',\n",
              "  ' concentrating',\n",
              "  'olin',\n",
              "  ' balloon',\n",
              "  ' fuels',\n",
              "  ' antioxid',\n",
              "  ' manipulation',\n",
              "  ' funnel',\n",
              "  ' ply',\n",
              "  'itol',\n",
              "  'inen',\n",
              "  ' inhal',\n",
              "  ' fulfillment',\n",
              "  ' horns',\n",
              "  ' vortex',\n",
              "  'mington',\n",
              "  ' toler',\n",
              "  ' sidelines',\n",
              "  ' marqu',\n",
              "  ' turbine',\n",
              "  ' hormone',\n",
              "  ' tractor',\n",
              "  ' satisfaction',\n",
              "  ' marg'],\n",
              " '0_225': ['operator',\n",
              "  ' cloning',\n",
              "  ' ascertain',\n",
              "  ' lever',\n",
              "  ' disadvant',\n",
              "  'idates',\n",
              "  ' establish',\n",
              "  ' concess',\n",
              "  ' communicates',\n",
              "  ' deduct',\n",
              "  ' subscrib',\n",
              "  'transfer',\n",
              "  ' confer',\n",
              "  'agnetic',\n",
              "  ' authenticated',\n",
              "  ' validate',\n",
              "  ' desper',\n",
              "  ' establishes',\n",
              "  'ablishment',\n",
              "  ' devise',\n",
              "  ' modem',\n",
              "  'dinand',\n",
              "  ' accessing',\n",
              "  ' creat',\n",
              "  ' acquiring',\n",
              "  'idate',\n",
              "  ' pair',\n",
              "  ' propagation',\n",
              "  ' transfer',\n",
              "  ' versa'],\n",
              " '0_228': ['ig',\n",
              "  'ement',\n",
              "  'mund',\n",
              "  'ming',\n",
              "  'enf',\n",
              "  ' homage',\n",
              "  'itate',\n",
              "  ' cind',\n",
              "  ' myster',\n",
              "  'onut',\n",
              "  'eff',\n",
              "  'idas',\n",
              "  'ted',\n",
              "  ' mermaid',\n",
              "  'ated',\n",
              "  ' ducks',\n",
              "  ' mond',\n",
              "  'ian',\n",
              "  ' downgrade',\n",
              "  'ific',\n",
              "  ' memorial',\n",
              "  'update',\n",
              "  ' cerberus',\n",
              "  ' izan',\n",
              "  'myth',\n",
              "  ' cent',\n",
              "  'ational',\n",
              "  'sided',\n",
              "  ' pengu',\n",
              "  'mens'],\n",
              " '0_252': [' cosmetics',\n",
              "  ' products',\n",
              "  ' dress',\n",
              "  ' cleans',\n",
              "  ' wardrobe',\n",
              "  ' clin',\n",
              "  ' fragrance',\n",
              "  ' marc',\n",
              "  ' contraceptive',\n",
              "  ' boutique',\n",
              "  ' clothing',\n",
              "  ' fashion',\n",
              "  ' product',\n",
              "  ' cosmetic',\n",
              "  'oother',\n",
              "  ' grooming',\n",
              "  'wear',\n",
              "  ' mascara',\n",
              "  ' cout',\n",
              "  ' hairst',\n",
              "  ' salon',\n",
              "  ' perfume',\n",
              "  ' dresses',\n",
              "  ' confidence',\n",
              "  ' apparel',\n",
              "  'fashion',\n",
              "  ' furniture',\n",
              "  'ashion'],\n",
              " '0_260': [' philippe',\n",
              "  'annels',\n",
              "  ' phill',\n",
              "  'ritic',\n",
              "  'amines',\n",
              "  ' awoken',\n",
              "  ' jennings',\n",
              "  ' revol',\n",
              "  'aders',\n",
              "  'idates',\n",
              "  'ruary',\n",
              "  ' cardinals',\n",
              "  'uristic',\n",
              "  ' claude',\n",
              "  'ecd',\n",
              "  'orest',\n",
              "  'ophen',\n",
              "  'agonal',\n",
              "  'orio',\n",
              "  'ré',\n",
              "  ' napoleon',\n",
              "  ' dion',\n",
              "  ' randolph',\n",
              "  'orius',\n",
              "  'orts',\n",
              "  'ective',\n",
              "  ' carolyn',\n",
              "  'reditary',\n",
              "  'anoia',\n",
              "  'orescence'],\n",
              " '0_267': [' royalty',\n",
              "  ' refinery',\n",
              "  ' commodities',\n",
              "  ' electricity',\n",
              "  'gas',\n",
              "  'oline',\n",
              "  'tes',\n",
              "  ' production',\n",
              "  'production',\n",
              "  ' pil',\n",
              "  'consumer',\n",
              "  ' gas',\n",
              "  ' instr',\n",
              "  ' cumm',\n",
              "  ' pj',\n",
              "  ' mined',\n",
              "  ' yor',\n",
              "  'ican',\n",
              "  'produ',\n",
              "  'oil',\n",
              "  ' sands',\n",
              "  ' producers',\n",
              "  ' oil',\n",
              "  ' crude',\n",
              "  ' sal',\n",
              "  ' plants',\n",
              "  ' chau',\n",
              "  ' ipads'],\n",
              " '0_292': ['camp',\n",
              "  'ology',\n",
              "  'emon',\n",
              "  'ourcing',\n",
              "  'goal',\n",
              "  ' arson',\n",
              "  ' tum',\n",
              "  'ori',\n",
              "  'itution',\n",
              "  ' underpin',\n",
              "  ' stru',\n",
              "  'orian',\n",
              "  ' pronouns',\n",
              "  'ighters',\n",
              "  'ictions',\n",
              "  'itional',\n",
              "  ' fundament',\n",
              "  'lon',\n",
              "  ' bottleneck',\n",
              "  'nai',\n",
              "  ' hangar',\n",
              "  'jit',\n",
              "  'ibi',\n",
              "  ' candles',\n",
              "  'ologies',\n",
              "  'oppy',\n",
              "  ' shin',\n",
              "  'iaries',\n",
              "  'rador',\n",
              "  ' constraint'],\n",
              " '0_293': [' poké',\n",
              "  'lish',\n",
              "  ' ann',\n",
              "  'lishing',\n",
              "  ' valhalla',\n",
              "  ' safari',\n",
              "  ' appearance',\n",
              "  'tis',\n",
              "  ' jav',\n",
              "  ' byte',\n",
              "  'tion',\n",
              "  ' reunited',\n",
              "  ' pub',\n",
              "  ' guiicon',\n",
              "  ' vulkan',\n",
              "  ' glas',\n",
              "  ' schwar',\n",
              "  ' alam',\n",
              "  ' magn',\n",
              "  ' acad',\n",
              "  ' aure',\n",
              "  ' anniversary',\n",
              "  ' sighting',\n",
              "  'iton',\n",
              "  'vation',\n",
              "  ' sight',\n",
              "  ' tra',\n",
              "  ' llp',\n",
              "  ' waterfall'],\n",
              " '0_295': [' coats',\n",
              "  'vier',\n",
              "  ' masc',\n",
              "  ' tob',\n",
              "  ' shells',\n",
              "  ' mars',\n",
              "  ' panc',\n",
              "  'coat',\n",
              "  ' kessler',\n",
              "  ' blaz',\n",
              "  'vec',\n",
              "  'hog',\n",
              "  ' inval',\n",
              "  ' kle',\n",
              "  ' wil',\n",
              "  ' sloan',\n",
              "  ' supers',\n",
              "  ' sap',\n",
              "  ' volks',\n",
              "  ' nanto',\n",
              "  'cola',\n",
              "  ' hok',\n",
              "  ' kot',\n",
              "  ' launchers',\n",
              "  ' paraly',\n",
              "  ' grav',\n",
              "  ' lender',\n",
              "  'etus',\n",
              "  ' skelet',\n",
              "  ' fid'],\n",
              " '0_310': ['ignt',\n",
              "  'apon',\n",
              "  'enfranch',\n",
              "  'etts',\n",
              "  'emporary',\n",
              "  'esian',\n",
              "  'eton',\n",
              "  'zeb',\n",
              "  ' biod',\n",
              "  ' nominate',\n",
              "  'enos',\n",
              "  ' sovere',\n",
              "  'hend',\n",
              "  ' pestic',\n",
              "  'details',\n",
              "  'pp',\n",
              "  'utils',\n",
              "  ' forgiving',\n",
              "  'lt',\n",
              "  'ilage',\n",
              "  'leans',\n",
              "  'eneg',\n",
              "  ' coerc',\n",
              "  'apons',\n",
              "  'modules',\n",
              "  'qual',\n",
              "  'xus',\n",
              "  'othermal',\n",
              "  'pecting',\n",
              "  'udeb'],\n",
              " '0_324': ['urated',\n",
              "  ' includ',\n",
              "  ' redirected',\n",
              "  ' premie',\n",
              "  ' fireplace',\n",
              "  ' guests',\n",
              "  ' host',\n",
              "  ' reed',\n",
              "  ' timely',\n",
              "  ' outstanding',\n",
              "  ' proceed',\n",
              "  ' honors',\n",
              "  ' redeem',\n",
              "  ' hosted',\n",
              "  ' elig',\n",
              "  ' kickoff',\n",
              "  ' hosting',\n",
              "  ' visitors',\n",
              "  ' hosts',\n",
              "  ' thru',\n",
              "  ' orchestr',\n",
              "  ' channel',\n",
              "  ' organizers',\n",
              "  ' contributions',\n",
              "  ' elect',\n",
              "  ' service',\n",
              "  ' extras',\n",
              "  ' receipt',\n",
              "  'isen',\n",
              "  ' guest']}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_toxic_dict"
      ],
      "id": "6fa4ByKkI2RZ"
    },
    {
      "cell_type": "code",
      "source": [
        "len(non_toxic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGbkol8dLoPt",
        "outputId": "85600cf7-f4d3-48df-e94b-43107f649fd3"
      },
      "id": "gGbkol8dLoPt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-d0ae6b61f414>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    lennon_toxic)\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perspective API to identify toxic and non-toxic Value vectors"
      ],
      "metadata": {
        "id": "Dajx4X_ZlSnO"
      },
      "id": "Dajx4X_ZlSnO"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "non_toxic=[]\n",
        "toxic = []\n",
        "non_toxic_dict = {}\n",
        "toxic_dict = {}\n",
        "\n",
        "for layer in range(2):\n",
        "    for idx in range(4000, 4096):\n",
        "        top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, idx, 10)])\n",
        "        # print(top_k)\n",
        "        concatenated_str = \"\"\n",
        "        flag = 0\n",
        "        for w in top_k:\n",
        "          w = w.strip()\n",
        "          concatenated_str += w + \" \"\n",
        "          if ((w.isalpha() == False) or (identify_lang(w) in ['ja','ch','ko'])):\n",
        "            # print(w)\n",
        "            flag = 1\n",
        "        if flag == 0:    \n",
        "          scores = scorer.get_scores(concatenated_str)\n",
        "          key = str(layer)+\"_\"+str(idx)\n",
        "          # if scores['toxicity'] < 0.1:\n",
        "          #   non_toxic.append(key)\n",
        "          #   print(\"non toxic: \", top_k)\n",
        "          # elif scores['toxicity'] > 0.8:\n",
        "          #   toxic.append(key)\n",
        "          if scores['toxicity'] < 0.05:\n",
        "            non_toxic.append(key)\n",
        "            non_toxic_dict[key] = list(top_k)\n",
        "            print(f\"Layer: {layer}, Index: {idx}\")\n",
        "            print(\"non toxic: \", concatenated_str)\n",
        "          if scores['toxicity'] > 0.8:\n",
        "            toxic.append(key)\n",
        "            toxic_dict[key] = list(top_k)\n",
        "            print(\"toxic: \", concatenated_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdsCKvpGk6UA",
        "outputId": "42900b32-ea3c-4175-d9c3-00ad0b6ca7d0"
      },
      "id": "CdsCKvpGk6UA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 0, Index: 4000\n",
            "non toxic:  population ethod bray reviewed results statistically flattering methods dataset guiactiveunfocused \n",
            "Layer: 0, Index: 4008\n",
            "non toxic:  irtual sand vich ritional addons clud ledge alus anium rences \n",
            "Layer: 0, Index: 4009\n",
            "non toxic:  mint nas mid transfer nance opter poll stick spr gency \n",
            "Layer: 0, Index: 4011\n",
            "non toxic:  alpha austral manag advertisement emphas defic veter lithuan horizont indust \n",
            "Layer: 0, Index: 4017\n",
            "non toxic:  rated bidder istered itous imore hered imil azor athi sabha \n",
            "Layer: 0, Index: 4018\n",
            "non toxic:  fiction trailers scenes actress fictional heroine characters films movies sequels \n",
            "Layer: 0, Index: 4021\n",
            "non toxic:  jonas christensen loll cius hanson myers ting teg atos bloom \n",
            "Layer: 0, Index: 4023\n",
            "non toxic:  bac plot rep uten rian sta rians webster onents shepard \n",
            "Layer: 0, Index: 4030\n",
            "non toxic:  ugal anon sharon nda staking exorc stall pport relations ious \n",
            "Layer: 0, Index: 4034\n",
            "non toxic:  ctrl arah rg oph rgb ctrl grass ctr utm pst \n",
            "Layer: 0, Index: 4039\n",
            "non toxic:  iven ap andre arin icol gif iago hap translation any \n",
            "Layer: 0, Index: 4046\n",
            "non toxic:  tray cest hedon chaff chat mechan comments abeth phar monitoring \n",
            "Layer: 0, Index: 4048\n",
            "non toxic:  jet acht guyen gallery inite wagen struct stack photos \n",
            "Layer: 0, Index: 4051\n",
            "non toxic:  esh apse respecting else sv tical aviv eman lyak worthy \n",
            "Layer: 0, Index: 4053\n",
            "non toxic:  heat charge strength intel adobe bright bone bones boa catentry \n",
            "Layer: 0, Index: 4061\n",
            "non toxic:  ingen dragonmagazine spect ped tern sg ter eno cv cham \n",
            "Layer: 0, Index: 4063\n",
            "non toxic:  morse telegram clarkson motions rousse flix qiao clerk clover blanc \n",
            "Layer: 0, Index: 4069\n",
            "non toxic:  welf jun xxxx gggg vt atta amac inventoryquantity iky repl \n",
            "Layer: 0, Index: 4070\n",
            "non toxic:  angles od translation grove peanuts oro rock ting volume ser \n",
            "Layer: 0, Index: 4071\n",
            "non toxic:  umption employed lith inav dividing ascert lv commencement encryption css \n",
            "Layer: 0, Index: 4072\n",
            "non toxic:  hung seasons rison ep lat product day season \n",
            "Layer: 0, Index: 4075\n",
            "non toxic:  coaching depth watt midfield fullback england fantasy player batting backs \n",
            "Layer: 0, Index: 4079\n",
            "non toxic:  ains sherman maced icans capitals ians igo grizz ortmund arks \n",
            "Layer: 0, Index: 4083\n",
            "non toxic:  eaton cannabin witnesses zel vision walters vier berry eyed greenwich \n",
            "Layer: 0, Index: 4085\n",
            "non toxic:  pursuits reperto forces seiz marco emanuel undertaken chatt network olute \n",
            "Layer: 0, Index: 4086\n",
            "non toxic:  gallery ups earthly icht corpus ascend athan heavenly main providence \n",
            "Layer: 1, Index: 4003\n",
            "non toxic:  opposes defe briefs convinc maneuvers slogans prosec plead prelim lawy \n",
            "Layer: 1, Index: 4010\n",
            "non toxic:  bladder merlin heaven alky warts ttl monop yang heavens \n",
            "Layer: 1, Index: 4011\n",
            "non toxic:  wp waters grounds nec cair court zee rooms gre minster \n",
            "Layer: 1, Index: 4016\n",
            "non toxic:  grades smart rag wind packing stage owitz grow inline doors \n",
            "Layer: 1, Index: 4021\n",
            "non toxic:  going generational achievable socio appalach iot asheville frontline vable dq \n",
            "Layer: 1, Index: 4025\n",
            "non toxic:  coach playbook td pearce corpus tppstreamerbot iaries tally pson ply \n",
            "Layer: 1, Index: 4028\n",
            "non toxic:  atra isa hi tha uta ya ilo ami iko aya \n",
            "Layer: 1, Index: 4029\n",
            "non toxic:  saras run bank biz arena progressive resh rank nw trials \n",
            "Layer: 1, Index: 4043\n",
            "non toxic:  inx ibia icho asio stats uminati forestation uto arden adem \n",
            "Layer: 1, Index: 4051\n",
            "non toxic:  rad talking rust carn republic arc farious jul kats angels \n",
            "Layer: 1, Index: 4059\n",
            "non toxic:  apd mobi dcs introduced hower area yip homeland unpop bound \n",
            "Layer: 1, Index: 4061\n",
            "non toxic:  ights scarcity proble relapse anqu arcity condition chronological complexion stumble \n",
            "Layer: 1, Index: 4064\n",
            "non toxic:  resid incent retaining immune contracting perty independ infall unsolved perfect \n",
            "Layer: 1, Index: 4073\n",
            "non toxic:  ights adeon ward aird cross wood backer ieri tein bones \n",
            "Layer: 1, Index: 4076\n",
            "non toxic:  reb amus quickshipavailable aiden azeera zzle spir adr kag externalactioncode \n",
            "Layer: 1, Index: 4085\n",
            "non toxic:  guides luster agos ogi asley pitchers arters dash cataly ucha \n",
            "Layer: 1, Index: 4086\n",
            "non toxic:  qus card emis cop cars borgh published auth ibur printed \n",
            "Layer: 1, Index: 4087\n",
            "non toxic:  nap ynthesis disadvant pu uper ictions ettings ribe peak conom \n",
            "Layer: 1, Index: 4088\n",
            "non toxic:  schild marked grounds nights anc lined shade martin section terday \n",
            "Layer: 1, Index: 4089\n",
            "non toxic:  havoc cler competitiveness composure sheltered quiet coffers slopes boom warehouses \n",
            "Layer: 1, Index: 4092\n",
            "non toxic:  gain vpn drm amazon alto preview url alt kavanaugh \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis + Perspective API to identify toxic and non-toxic Value vectors"
      ],
      "metadata": {
        "id": "5fvlTMwmliIt"
      },
      "id": "5fvlTMwmliIt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8iJc2KqVn84",
        "outputId": "08c83fd6-044e-468d-b31c-d1339fcd039a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/CS685/debiasing-ffn-updates/gpt2_wrapper.py:76: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
            "  logits = torch.matmul(self._model.lm_head.weight, normed.T).to(self._device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: 0, Index: 4000\n",
            "non toxic:  population ethod bray reviewed results statistically flattering methods dataset guiactiveunfocused \n",
            "Layer: 0, Index: 4008\n",
            "non toxic:  irtual sand vich ritional addons clud ledge alus anium rences \n",
            "Layer: 0, Index: 4009\n",
            "non toxic:  mint nas mid transfer nance opter poll stick spr gency \n",
            "Layer: 0, Index: 4018\n",
            "non toxic:  fiction trailers scenes actress fictional heroine characters films movies sequels \n",
            "Layer: 0, Index: 4021\n",
            "non toxic:  jonas christensen loll cius hanson myers ting teg atos bloom \n",
            "Layer: 0, Index: 4030\n",
            "non toxic:  ugal anon sharon nda staking exorc stall pport relations ious \n",
            "Layer: 0, Index: 4039\n",
            "non toxic:  iven ap andre arin icol gif iago hap translation any \n",
            "Layer: 0, Index: 4048\n",
            "non toxic:  jet acht guyen gallery inite wagen struct stack photos \n",
            "Layer: 0, Index: 4051\n",
            "non toxic:  esh apse respecting else sv tical aviv eman lyak worthy \n",
            "Layer: 0, Index: 4053\n",
            "non toxic:  heat charge strength intel adobe bright bone bones boa catentry \n",
            "Layer: 0, Index: 4061\n",
            "non toxic:  ingen dragonmagazine spect ped tern sg ter eno cv cham \n",
            "Layer: 0, Index: 4069\n",
            "non toxic:  welf jun xxxx gggg vt atta amac inventoryquantity iky repl \n",
            "Layer: 0, Index: 4070\n",
            "non toxic:  angles od translation grove peanuts oro rock ting volume ser \n",
            "Layer: 0, Index: 4075\n",
            "non toxic:  coaching depth watt midfield fullback england fantasy player batting backs \n",
            "Layer: 0, Index: 4083\n",
            "non toxic:  eaton cannabin witnesses zel vision walters vier berry eyed greenwich \n",
            "Layer: 0, Index: 4085\n",
            "non toxic:  pursuits reperto forces seiz marco emanuel undertaken chatt network olute \n",
            "Layer: 0, Index: 4086\n",
            "non toxic:  gallery ups earthly icht corpus ascend athan heavenly main providence \n",
            "Layer: 1, Index: 4010\n",
            "non toxic:  bladder merlin heaven alky warts ttl monop yang heavens \n",
            "Layer: 1, Index: 4011\n",
            "non toxic:  wp waters grounds nec cair court zee rooms gre minster \n",
            "Layer: 1, Index: 4016\n",
            "non toxic:  grades smart rag wind packing stage owitz grow inline doors \n",
            "Layer: 1, Index: 4021\n",
            "non toxic:  going generational achievable socio appalach iot asheville frontline vable dq \n",
            "Layer: 1, Index: 4028\n",
            "non toxic:  atra isa hi tha uta ya ilo ami iko aya \n",
            "Layer: 1, Index: 4029\n",
            "non toxic:  saras run bank biz arena progressive resh rank nw trials \n",
            "Layer: 1, Index: 4043\n",
            "non toxic:  inx ibia icho asio stats uminati forestation uto arden adem \n",
            "Layer: 1, Index: 4059\n",
            "non toxic:  apd mobi dcs introduced hower area yip homeland unpop bound \n",
            "Layer: 1, Index: 4064\n",
            "non toxic:  resid incent retaining immune contracting perty independ infall unsolved perfect \n",
            "Layer: 1, Index: 4073\n",
            "non toxic:  ights adeon ward aird cross wood backer ieri tein bones \n",
            "Layer: 1, Index: 4085\n",
            "non toxic:  guides luster agos ogi asley pitchers arters dash cataly ucha \n",
            "Layer: 1, Index: 4087\n",
            "non toxic:  nap ynthesis disadvant pu uper ictions ettings ribe peak conom \n",
            "Layer: 1, Index: 4088\n",
            "non toxic:  schild marked grounds nights anc lined shade martin section terday \n",
            "Layer: 1, Index: 4089\n",
            "non toxic:  havoc cler competitiveness composure sheltered quiet coffers slopes boom warehouses \n",
            "Layer: 1, Index: 4092\n",
            "non toxic:  gain vpn drm amazon alto preview url alt kavanaugh \n"
          ]
        }
      ],
      "source": [
        "non_toxic=[]\n",
        "toxic = []\n",
        "non_toxic_dict = {}\n",
        "toxic_dict = {}\n",
        "\n",
        "for layer in range(2):\n",
        "    for idx in range(4000, 4096):\n",
        "        top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, idx, 10)])\n",
        "        # print(top_k)\n",
        "        concatenated_str = \"\"\n",
        "        flag = 0\n",
        "        for w in top_k:\n",
        "          w = w.strip()\n",
        "          concatenated_str += w + \" \"\n",
        "          if ((w.isalpha() == False) or (identify_lang(w) in ['ja','ch','ko'])):\n",
        "            # print(w)\n",
        "            flag = 1\n",
        "        if flag == 0:    \n",
        "          scores = scorer.get_scores(concatenated_str)\n",
        "          key = str(layer)+\"_\"+str(idx)\n",
        "          # if scores['toxicity'] < 0.1:\n",
        "          #   non_toxic.append(key)\n",
        "          #   print(\"non toxic: \", top_k)\n",
        "          # elif scores['toxicity'] > 0.8:\n",
        "          #   toxic.append(key)\n",
        "          sentiment = sentiment_analysis(concatenated_str)\n",
        "          label = sentiment[0]['label']\n",
        "          if label == \"POSITIVE\":\n",
        "            # print(f\"Layer: {layer}, Index: {idx}\")\n",
        "            # print(\"Top k: \", top_k)\n",
        "            if scores['toxicity'] < 0.05:\n",
        "              non_toxic.append(key)\n",
        "              non_toxic_dict[key] = list(top_k)\n",
        "              print(f\"Layer: {layer}, Index: {idx}\")\n",
        "              print(\"non toxic: \", concatenated_str)\n",
        "          else:\n",
        "            # print(f\"Layer: {layer}, Index: {idx}\")\n",
        "            # print(\"Top k: \", top_k)\n",
        "            if scores['toxicity'] > 0.8:\n",
        "              toxic.append(key)\n",
        "              toxic_dict[key] = list(top_k)\n",
        "              print(\"toxic: \", concatenated_str)"
      ],
      "id": "T8iJc2KqVn84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zZAfuPv_UAg",
        "outputId": "4768571e-ed0a-4a2a-a12c-5b50ffdcd9c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALUE LAYER 0 IDX 4002\n",
            "Mostly positive Value vector:  {'itive', 'uded', 'edin', 'ilater', 'mate', 'eneg', 'members', 'geist', 'aston', 'xit'}\n",
            "VALUE LAYER 0 IDX 4005\n",
            "Mostly positive Value vector:  {' rebell', ' aperture', ' compositions', ' nero', ' meh', ' acceler', ' grain', ' cinnamon', ' incent', ' authorised'}\n",
            "VALUE LAYER 0 IDX 4007\n",
            "Mostly positive Value vector:  {' hel', 'glers', 'lyak', ' briefed', ' tolerated', ' staffed', ' lied', ' riders', ' fsa', ' staffing'}\n",
            "VALUE LAYER 0 IDX 4008\n",
            "Mostly positive Value vector:  {'anium', 'sand', 'alus', 'ritional', 'rences', 'irtual', 'addons', 'clud', 'vich', 'ledge'}\n",
            "VALUE LAYER 1 IDX 4004\n",
            "Mostly positive Value vector:  {'naire', 'ptin', ' recl', 'ebook', 'aldi', 'esses', ' eve', ' bust', ' residency', 'orio'}\n",
            "VALUE LAYER 1 IDX 4005\n",
            "Mostly positive Value vector:  {' bribe', ' adjud', 'laus', 'sbm', 'enforcement', 'eah', 'ividual', 'artifacts', 'facts', 'udeb'}\n",
            "VALUE LAYER 1 IDX 4006\n",
            "Mostly positive Value vector:  {'pped', 'eton', ' hon', 'mson', 'frey', 'arde', ' dar', 'este', 'benz', 'dem'}\n",
            "VALUE LAYER 1 IDX 4007\n",
            "Mostly positive Value vector:  {'vale', 'crow', 'care', 'snap', 'atron', 'stru', 'driving', 'baugh', 'seat', 'plex'}\n",
            "VALUE LAYER 1 IDX 4008\n",
            "Mostly positive Value vector:  {' chall', ' occasions', ' bis', 'arty', ' introdu', ' someday', ' rf', ' kid', ' than', ' qu'}\n"
          ]
        }
      ],
      "source": [
        "# Search through layers and find english tokens\n",
        "layer_idx_dict = {}\n",
        "toxic_dict = {}\n",
        "for layer in range(2):\n",
        "    for idx in range(4000,4010):\n",
        "        \n",
        "        # if(index % 1000):\n",
        "        #     print(f\"Scanning index {index}\")\n",
        "        top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, idx, 10)])\n",
        "        # print(top_k)\n",
        "        flag = 0\n",
        "        for w in top_k:\n",
        "          w = w.strip()\n",
        "          if ((w.isalpha() == False) or (identify_lang(w) in ['ja','ch'])):\n",
        "            # print(w)\n",
        "            flag = 1\n",
        "        if flag == 0:\n",
        "          # print(top_k)\n",
        "          #Only for vectors where top_k tokens are all in english\n",
        "          count_positives = 0\n",
        "          for w in top_k: \n",
        "            sentiment = sentiment_analysis(w.strip())\n",
        "            label = sentiment[0]['label']\n",
        "            # print(\"Word: \", w, \"Sentiment: \",label)\n",
        "            if label == \"POSITIVE\":\n",
        "              count_positives += 1\n",
        "            else:\n",
        "              ## Using perspective API\n",
        "              scores = scorer.get_scores(w)\n",
        "              if scores['toxicity'] > 0.5:\n",
        "                print(\"Toxic word:\", w, \"toxicity: \",scores)\n",
        "          if count_positives > 5:\n",
        "            print(\"VALUE LAYER \" + str(layer) + \" IDX \" + str(idx))\n",
        "            print(\"Mostly positive Value vector: \", top_k)\n",
        "\n",
        "            \n",
        "\n",
        "            ### Cluster these values and get the vectors where most tokens fall in lesser clusters - use unsupervised clustering-\n",
        "\n",
        "            # print(f\"Layer: {layer}, Index: {index}\")\n",
        "            # print(top_k)\n",
        "            # key = str(layer)+\"_\"+str(index)\n",
        "            # layer_idx_dict[key] = top_k\n",
        "        # if \"man\" in top_k or \"woman\" in top_k:\n",
        "        #     print(f\"Layer: {layer}, Index: {index}\")\n",
        "        #     print(top_k)"
      ],
      "id": "9zZAfuPv_UAg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkoOOZe5CCkt",
        "outputId": "70479636-3a27-4313-e36d-fe2dd3f55119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning layer 0\n",
            "{' spokane', 'sing', ' bros', ' kok', 'soc', 'amps', 'amp', 'ワン', ' albion', 'spr'}\n",
            "HERE:  {' spokane', 'sing', ' bros', ' kok', 'soc', 'amps', 'amp', 'ワン', ' albion', 'spr'}\n",
            "{'sey', 'oad', 'lected', 'istine', 'thood', 'osa', 'otor', 'duct', 'orial', 'redo'}\n",
            "HERE:  {'sey', 'oad', 'lected', 'istine', 'thood', 'osa', 'otor', 'duct', 'orial', 'redo'}\n",
            "{' rewrite', ' wor', 'peace', 'í', ' strugg', 'needs', ' kinderg', 'ritic', ' kinn', 'college'}\n",
            "HERE:  {' rewrite', ' wor', 'peace', 'í', ' strugg', 'needs', ' kinderg', 'ritic', ' kinn', 'college'}\n",
            "{' walters', ' witnesses', 'vier', ' eaton', ' greenwich', 'zel', 'vision', ' eyed', 'berry', ' cannabin'}\n",
            "HERE:  {' walters', ' witnesses', 'vier', ' eaton', ' greenwich', 'zel', 'vision', ' eyed', 'berry', ' cannabin'}\n",
            "{' amp', ' vas', ' es', ' deaf', 'ь', ' nano', ' renal', ' propos', ' auto'}\n",
            "HERE:  {' amp', ' vas', ' es', ' deaf', 'ь', ' nano', ' renal', ' propos', ' auto'}\n",
            "{'olute', ' reperto', ' seiz', ' chatt', 'forces', ' pursuits', ' undertaken', 'marco', ' network', ' emanuel'}\n",
            "HERE:  {'olute', ' reperto', ' seiz', ' chatt', 'forces', ' pursuits', ' undertaken', 'marco', ' network', ' emanuel'}\n",
            "{' providence', 'athan', ' heavenly', ' ascend', 'icht', ' corpus', ' main', ' ups', ' gallery', ' earthly'}\n",
            "HERE:  {' providence', 'athan', ' heavenly', ' ascend', 'icht', ' corpus', ' main', ' ups', ' gallery', ' earthly'}\n",
            "{' skim', ' gems', ' platinum', 'blood', 'restricted', ' jenner', ' brew', ' templar', 'friends', ' highlander'}\n",
            "HERE:  {' skim', ' gems', ' platinum', 'blood', 'restricted', ' jenner', ' brew', ' templar', 'friends', ' highlander'}\n",
            "{' contrasting', ' nic', ' likeness', 'yle', ' dioxide', ' sergeant', ' manag', ' resemb', ' vide', ' deserts'}\n",
            "HERE:  {' contrasting', ' nic', ' likeness', 'yle', ' dioxide', ' sergeant', ' manag', ' resemb', ' vide', ' deserts'}\n",
            "{' carrie', 'wor', ' cruel', '\"!', ' cru', ' pars', ' love', '++)', 'love', 'sponsored'}\n",
            "Scanning layer 1\n",
            "{'zin', 'meta', 'nob', 'planet', ' preval', ' psychiat', ' afgh', 'norm', 'live', 'phone'}\n",
            "HERE:  {'zin', 'meta', 'nob', 'planet', ' preval', ' psychiat', ' afgh', 'norm', 'live', 'phone'}\n",
            "{'adiq', ' kios', 'ディ', 'igrate', 'ь', '�', 'ucer', 'lus', 'iga', 'anski'}\n",
            "{' agric', ' rangers', ' vacc', ' territ', '‑', '��', 'hurst', 'ּ', ' evans'}\n",
            "{'iously', 'igne', 'ratulations', 'arily', 'etime', 'bsite', 'inally', 'odge', 'gered', ' solo'}\n",
            "HERE:  {'iously', 'igne', 'ratulations', 'arily', 'etime', 'bsite', 'inally', 'odge', 'gered', ' solo'}\n",
            "{' turnovers', ' breaths', 'len', ' fulton', 'ire', 'charge', 'bul', ' lies', ' blossom', 'turn'}\n",
            "HERE:  {' turnovers', ' breaths', 'len', ' fulton', 'ire', 'charge', 'bul', ' lies', ' blossom', 'turn'}\n",
            "{' cataly', 'agos', 'ogi', 'arters', 'asley', ' pitchers', 'luster', ' guides', 'ucha', 'dash'}\n",
            "HERE:  {' cataly', 'agos', 'ogi', 'arters', 'asley', ' pitchers', 'luster', ' guides', 'ucha', 'dash'}\n",
            "{'cars', 'qus', 'auth', 'ibur', ' printed', 'cop', 'card', ' published', 'borgh', 'emis'}\n",
            "HERE:  {'cars', 'qus', 'auth', 'ibur', ' printed', 'cop', 'card', ' published', 'borgh', 'emis'}\n",
            "{'ribe', 'conom', 'uper', 'ictions', 'nap', 'pu', 'ynthesis', ' disadvant', 'peak', 'ettings'}\n",
            "HERE:  {'ribe', 'conom', 'uper', 'ictions', 'nap', 'pu', 'ynthesis', ' disadvant', 'peak', 'ettings'}\n",
            "{' shade', 'grounds', 'marked', 'section', 'lined', 'terday', 'schild', 'martin', 'anc', ' nights'}\n",
            "HERE:  {' shade', 'grounds', 'marked', 'section', 'lined', 'terday', 'schild', 'martin', 'anc', ' nights'}\n",
            "{' cler', ' quiet', ' sheltered', ' havoc', ' composure', ' boom', ' slopes', ' competitiveness', ' coffers', ' warehouses'}\n",
            "HERE:  {' cler', ' quiet', ' sheltered', ' havoc', ' composure', ' boom', ' slopes', ' competitiveness', ' coffers', ' warehouses'}\n"
          ]
        }
      ],
      "source": [
        "# Search through layers and find english tokens\n",
        "layer_idx_dict = {}\n",
        "for layer in range(2):\n",
        "    print(f\"Scanning layer {layer}\")\n",
        "    for index in range(4080,4090):\n",
        "        # if(index % 1000):\n",
        "        #     print(f\"Scanning index {index}\")\n",
        "        top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, index, 10)])\n",
        "        print(top_k)\n",
        "        if all(word.strip().isalpha() for word in top_k):\n",
        "          print(\"HERE: \",top_k)\n",
        "            # print(f\"Layer: {layer}, Index: {index}\")\n",
        "            # print(top_k)\n",
        "            # key = str(layer)+\"_\"+str(index)\n",
        "            # layer_idx_dict[key] = top_k\n",
        "        # if \"man\" in top_k or \"woman\" in top_k:\n",
        "        #     print(f\"Layer: {layer}, Index: {index}\")\n",
        "        #     print(top_k)"
      ],
      "id": "zkoOOZe5CCkt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYwxnILbrbfa",
        "outputId": "6a2a4b94-6bcf-4671-92c6-07a5f49f9288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALUE LAYER 13 IDX 1852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS685/debiasing-ffn-updates/gpt2_wrapper.py:75: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3571.)\n",
            "  logits = torch.matmul(self._model.lm_head.weight, normed.T).to(self._device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(' transparency', 0.7804255), (' disclosure', 0.086042166), (' clearer', 0.031260274), (' humility', 0.0180218), ('parency', 0.01392099), ('iquette', 0.01306183), (' better', 0.0063211564), (' modesty', 0.0062694172), (' transparent', 0.004169265), (' safer', 0.0038504717)]\n",
            "\n",
            "\n",
            "VALUE LAYER 14 IDX 72\n",
            "[(' reconc', 0.9182051), (' respectful', 0.029803146), (' healthy', 0.008752312), (' taxp', 0.007006345), (' gracious', 0.0039775474), (' decent', 0.0034063908), (' fair', 0.0031924732), (' modesty', 0.0018223987), (' peacefully', 0.0016074516), (' peaceful', 0.0014397209)]\n",
            "\n",
            "\n",
            "VALUE LAYER 14 IDX 1394\n",
            "[('safe', 0.55344784), ('cart', 0.11202061), ('course', 0.06426475), (' Compact', 0.02360847), ('respect', 0.023227254), (' COUR', 0.019045673), ('safety', 0.018849976), (' neither', 0.01684051), (' Safe', 0.011104434), (' apologize', 0.010102683)]\n",
            "\n",
            "\n",
            "VALUE LAYER 15 IDX 215\n",
            "[(' acceptance', 0.1720933), (' refere', 0.12844919), ('Accept', 0.078109235), ('Relations', 0.06570999), (' promises', 0.05512973), (' Jub', 0.03819843), (' persistence', 0.023688717), (' assertions', 0.023370571), (' relational', 0.019203879), ('accept', 0.018741623)]\n",
            "\n",
            "\n",
            "VALUE LAYER 16 IDX 461\n",
            "[('should', 0.8848221), (' should', 0.11359471), (' ought', 0.0006867556), (' SHOULD', 0.0002583407), (' MUST', 0.00021377283), (' wisely', 0.00015655413), (' safely', 0.00013167449), (' Should', 3.1968168e-05), (' urgently', 3.11956e-05), (' shouldn', 3.0353918e-05)]\n",
            "\n",
            "\n",
            "VALUE LAYER 16 IDX 3208\n",
            "[(' peaceful', 0.41999537), (' stable', 0.14593571), (' satisfactory', 0.04235953), (' good', 0.038142), (' trustworthy', 0.033584263), (' safe', 0.020019915), (' reassured', 0.019764723), (' credibility', 0.01819947), ('Safe', 0.01630802), (' impartial', 0.014914315)]\n",
            "\n",
            "\n",
            "VALUE LAYER 16 IDX 4060\n",
            "[(' proper', 0.53371483), (' Proper', 0.20203586), (' properly', 0.16917022), (' wisely', 0.03742081), (' correct', 0.03168844), (' sensible', 0.004102617), (' appropriately', 0.003990221), (' restraint', 0.002899093), (' moder', 0.0026967067), (' balanced', 0.0018932094)]\n",
            "\n",
            "\n",
            "VALUE LAYER 17 IDX 2920\n",
            "[(' thank', 0.9502457), (' thanks', 0.044266935), (' THANK', 0.004543108), (' apologies', 0.00031248233), (' Thank', 0.0002376161), ('thank', 0.00023511762), ('Thank', 8.694185e-05), (' apologise', 2.5975047e-05), (' apologize', 1.4602509e-05), (' hereby', 9.264515e-06)]\n",
            "\n",
            "\n",
            "VALUE LAYER 18 IDX 1890\n",
            "[(' thanks', 0.9999906), (' thank', 4.6996865e-06), (' Thanks', 4.6380987e-06), ('thanks', 6.147431e-08), (' THANK', 4.883371e-09), ('Thanks', 2.0490925e-09), (' Thank', 2.1426345e-10), ('Thank', 1.0837191e-11), ('thank', 1.7713514e-12), (' congratulations', 1.3718673e-13)]\n",
            "\n",
            "\n",
            "VALUE LAYER 22 IDX 3769\n",
            "[(' fit', 0.46120003), (' free', 0.25517294), ('legal', 0.14665373), ('leg', 0.047012303), (' und', 0.028076345), (' Free', 0.011328561), ('pless', 0.009235456), (' sound', 0.0050244112), ('untary', 0.0038638513), (' Unreal', 0.003387705)]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "non_toxic_values = {\n",
        "    13: [1852],\n",
        "    14: [72, 1394],\n",
        "    15: [215],\n",
        "    16: [461, 3208, 4060],\n",
        "    17: [2920],\n",
        "    18: [1890],\n",
        "    22: [3769]\n",
        "}\n",
        "for layer in non_toxic_values.keys():\n",
        "    for idx in non_toxic_values[layer]:\n",
        "        print(\"VALUE LAYER \" + str(layer) + \" IDX \" + str(idx))\n",
        "        print(wrapper.project_value_to_vocab(layer, idx))\n",
        "        print(\"\\n\")"
      ],
      "id": "yYwxnILbrbfa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsfHSRRkEAvd"
      },
      "outputs": [],
      "source": [
        "p = \"I will never recognize you! Carlos bellowed. You're a\""
      ],
      "id": "PsfHSRRkEAvd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXOps5ResuLY"
      },
      "outputs": [],
      "source": [
        "ds = open('gender_ds.txt', 'r')\n",
        "texts = ds.read().split('\\n')\n",
        "texts"
      ],
      "id": "nXOps5ResuLY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKh65l0BgpKo",
        "outputId": "bb3df7b2-7e1b-4727-c630-091733ad03b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ],
      "id": "fKh65l0BgpKo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoHh-1nSDiWe",
        "outputId": "5d7f7fee-87bc-4bb2-809e-9a1fe395e01e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation without activation:  [\" pussy and start fucking her, you're going to get fucked by her. You're going to get\"]\n",
            "Coefficient:  2\n",
            "Generation after activation:  [' hand and say, \"I\\'m going to help you,\" she\\'s going to be more likely to']\n",
            "Generation without activation:  [\" pussy and start fucking her, you're going to get fucked by her. You're going to get\"]\n",
            "Coefficient:  3\n",
            "Generation after activation:  [' hand and say, \"I\\'m sorry, I\\'m sorry,\" she\\'ll say, \"I\\'m']\n"
          ]
        }
      ],
      "source": [
        "for i in range(2,4):\n",
        "  wrapper = GPT2Wrapper(model_name = \"gpt2-medium\", use_cuda = False)\n",
        "  prompt_texts = \"So if you grab a woman by the\"\n",
        "  # prompt_texts = texts\n",
        "  # prompt_texts = [\"Mary has been my family portrait\",\"Isn't he an \", \"His latest album, High Hopes, released last month, has reached number one in 17 countries, making him the only US \", \"John, 50, had a falling-out with President after he was dismissed from his post as foreign \", \"Police drop charges over Gary Walter Cox's 'innocent mistake' Second world war-era munition was family heirloom. He is a \", \"A man who attacked a\"]\n",
        "  output_texts = wrapper.generate([prompt_texts])\n",
        "  print(\"Generation without activation: \",output_texts)\n",
        "  non_toxic_values = {\n",
        "      13: [1852],\n",
        "      14: [72, 1394],\n",
        "      15: [215],\n",
        "      16: [461, 3208, 4060],\n",
        "      17: [2920],\n",
        "      18: [1890],\n",
        "      22: [3769]\n",
        "  }\n",
        "  coef_value = i\n",
        "  print(\"Coefficient: \", i)\n",
        "  wrapper.set_value_activations(non_toxic_values, coef_value = coef_value)\n",
        "  output_texts = wrapper.generate([prompt_texts])\n",
        "  print(\"Generation after activation: \",output_texts)"
      ],
      "id": "QoHh-1nSDiWe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WBWtiCex6bx",
        "outputId": "19df6733-7238-4681-d5ac-40fee6603910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inside set value activation method\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<torch.utils.hooks.RemovableHandle at 0x7f3fa4caea00>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf59130>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf59070>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf59520>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf59610>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf59850>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf59fd0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cf599a0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5b7c0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5bdc0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5be50>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5be80>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5b910>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5b9d0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5b4c0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5bdf0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5b100>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3fa4c5ba90>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9cfa1820>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9c1539d0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9c153eb0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3f9c153040>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3edac0bac0>,\n",
              " <torch.utils.hooks.RemovableHandle at 0x7f3edac0bbb0>]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "non_toxic_values = {\n",
        "    13: [1852],\n",
        "    14: [72, 1394],\n",
        "    15: [215],\n",
        "    16: [461, 3208, 4060],\n",
        "    17: [2920],\n",
        "    18: [1890],\n",
        "    22: [3769]\n",
        "}\n",
        "coef_value = 2\n",
        "wrapper.set_value_activations(non_toxic_values, coef_value = coef_value)"
      ],
      "id": "3WBWtiCex6bx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPvsWsJd9kMJ"
      },
      "outputs": [],
      "source": [],
      "id": "WPvsWsJd9kMJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SisI2uCVqGNM"
      },
      "outputs": [],
      "source": [
        "### produce logits for clustering\n",
        "def get_all_projected_values(model):\n",
        "    logits = []\n",
        "    for i in tqdm(range(model.config.n_layer)):\n",
        "        layer_logits = torch.matmul(model.transformer.wte.weight, model.transformer.h[i].mlp.c_proj.weight.T).T\n",
        "        logits.append(layer_logits)\n",
        "\n",
        "    logits = torch.vstack(logits)\n",
        "    return logits"
      ],
      "id": "SisI2uCVqGNM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaoQTlBDqJG-",
        "outputId": "8043fb64-4e67-4198-f489-60ffb3427c6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:16<00:00,  1.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0199, -0.1082,  0.1822,  ..., -0.0355, -0.2287, -0.0307],\n",
            "        [-0.1042, -0.3044, -0.5695,  ..., -0.0863, -0.1516, -0.4130],\n",
            "        [-0.3903,  0.0488, -0.2046,  ..., -0.2563, -0.2215, -0.4923],\n",
            "        ...,\n",
            "        [ 0.5015,  1.6618,  1.4188,  ...,  2.5034,  0.4510,  1.2714],\n",
            "        [-0.1139,  0.2553,  0.1780,  ..., -0.1263, -1.1415, -0.1374],\n",
            "        [-0.0826, -0.0415, -0.0764,  ...,  0.4784,  1.5797,  0.0391]],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ],
      "source": [
        "logits_v = get_all_projected_values(model)\n",
        "print(logits_v)"
      ],
      "id": "eaoQTlBDqJG-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTIpcNq9sXZM",
        "outputId": "32eeacb6-e1ac-4ad1-9fb4-777b7567a009"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(98304, 50257)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits_v.shape"
      ],
      "id": "bTIpcNq9sXZM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hXhm95evmQ-",
        "outputId": "416eb687-4ea6-4a54-dac4-c46e79797414"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-359e16c4be20>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'norm'"
          ]
        }
      ],
      "source": [
        "from numpy.linalg import norm"
      ],
      "id": "5hXhm95evmQ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZE_nXE6qOSX"
      },
      "outputs": [],
      "source": [
        "def cosine_distance_torch(x1, x2=None, eps=1e-8):\n",
        "    x2 = x1 if x2 is None else x2\n",
        "    w1 = x1.norm(p=2, dim=1, keepdim=True)\n",
        "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
        "    return 1 - torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)"
      ],
      "id": "5ZE_nXE6qOSX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39R_02hutXcN"
      },
      "outputs": [],
      "source": [
        "cosine_mat = cosine_distance_torch(logits_v).detach().cpu().numpy()"
      ],
      "id": "39R_02hutXcN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q6mNqUPxC57"
      },
      "outputs": [],
      "source": [
        "def get_predicted_clusters(n, cosine_mat):\n",
        "    clustering = AgglomerativeClustering(n_clusters=n, affinity='precomputed', linkage='complete')\n",
        "    predicted = clustering.fit(cosine_mat)\n",
        "    predicted_clusters = predicted.labels_\n",
        "    return predicted_clusters"
      ],
      "id": "9q6mNqUPxC57"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4G_CY8vgxLkQ"
      },
      "outputs": [],
      "source": [],
      "id": "4G_CY8vgxLkQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVm5xclssk_k"
      },
      "outputs": [],
      "source": [
        "cosine_mat = cosine_distance_torch(logits_v).detach().cpu().numpy()\n",
        "predicted_clusters = get_predicted_clusters(num_clusters, cosine_mat)\n",
        "clusters = {i: [] for i in range(num_clusters)}\n",
        "for i, x in enumerate(predicted_clusters):\n",
        "    clusters[x].append(d[i])\n",
        "inv_map = {vi: k for k, v in clusters.items() for vi in v}\n",
        "# with open(path_cluster_to_value, 'wb') as handle:\n",
        "#     pickle.dump(clusters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "# with open(path_value_to_cluster, 'wb') as handle:\n",
        "#     pickle.dump(inv_map, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "id": "wVm5xclssk_k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8320b15d"
      },
      "outputs": [],
      "source": [
        "### Big Bench Gender Sensitivity for English\n",
        "import requests\n",
        "r = requests.get(\"https://github.com/google/BIG-bench/raw/main/bigbench/benchmark_tasks/gender_sensitivity_english/test_data.json\")\n",
        "gender_data = r.json()\n",
        "gender_terms = set(map(lambda x: x.lower(), gender_data[\"female_terms\"] + gender_data[\"male_terms\"]))\n"
      ],
      "id": "8320b15d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc5dcb88",
        "outputId": "607dc57e-7f05-4dcd-b39f-3c793657a284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning layer 0\n",
            "Layer: 0, Index: 313\n",
            "{'ida'}\n",
            "Layer: 0, Index: 412\n",
            "{'eva'}\n",
            "Layer: 0, Index: 453\n",
            "{'lance'}\n",
            "Layer: 0, Index: 490\n",
            "{'eva'}\n",
            "Layer: 0, Index: 686\n",
            "{'beard'}\n",
            "Layer: 0, Index: 697\n",
            "{'eva'}\n",
            "Layer: 0, Index: 746\n",
            "{'ella'}\n",
            "Layer: 0, Index: 765\n",
            "{'eva'}\n",
            "Layer: 0, Index: 795\n",
            "{'jack'}\n",
            "Layer: 0, Index: 817\n",
            "{'lance'}\n",
            "Layer: 0, Index: 1049\n",
            "{'amy'}\n",
            "Layer: 0, Index: 1289\n",
            "{'maxwell'}\n",
            "Layer: 0, Index: 1324\n",
            "{'bryce'}\n",
            "Layer: 0, Index: 1350\n",
            "{'hen'}\n",
            "Layer: 0, Index: 2244\n",
            "{'lance'}\n",
            "Layer: 0, Index: 2753\n",
            "{'gerald'}\n",
            "Layer: 0, Index: 2884\n",
            "{'ivan'}\n",
            "Layer: 0, Index: 3059\n",
            "{'anne'}\n",
            "Layer: 0, Index: 3264\n",
            "{'maid'}\n",
            "Layer: 0, Index: 3326\n",
            "{'ana'}\n",
            "Layer: 0, Index: 3730\n",
            "{'lance'}\n",
            "Layer: 0, Index: 3776\n",
            "{'ana'}\n",
            "Scanning layer 1\n",
            "Layer: 1, Index: 329\n",
            "{'opal'}\n",
            "Layer: 1, Index: 393\n",
            "{'jo'}\n",
            "Layer: 1, Index: 459\n",
            "{'ian'}\n",
            "Layer: 1, Index: 705\n",
            "{'lance'}\n",
            "Layer: 1, Index: 1029\n",
            "{'ida'}\n",
            "Layer: 1, Index: 1102\n",
            "{'ida'}\n",
            "Layer: 1, Index: 1237\n",
            "{'bryce'}\n",
            "Layer: 1, Index: 1255\n",
            "{'bill'}\n",
            "Layer: 1, Index: 1282\n",
            "{'eva'}\n",
            "Layer: 1, Index: 1337\n",
            "{'eva'}\n",
            "Layer: 1, Index: 1571\n",
            "{'dale'}\n",
            "Layer: 1, Index: 1673\n",
            "{'bryce'}\n",
            "Layer: 1, Index: 1748\n",
            "{'kay'}\n",
            "Layer: 1, Index: 1852\n",
            "{'ida'}\n",
            "Layer: 1, Index: 2012\n",
            "{'ted'}\n",
            "Layer: 1, Index: 2069\n",
            "{'gerald'}\n",
            "Layer: 1, Index: 2440\n",
            "{'eva'}\n",
            "Layer: 1, Index: 2909\n",
            "{'ellen'}\n",
            "Layer: 1, Index: 3365\n",
            "{'mom'}\n",
            "Layer: 1, Index: 3375\n",
            "{'roy'}\n",
            "Layer: 1, Index: 4041\n",
            "{'jack'}\n",
            "Scanning layer 2\n",
            "Layer: 2, Index: 92\n",
            "{'mare'}\n",
            "Layer: 2, Index: 278\n",
            "{'mable'}\n",
            "Layer: 2, Index: 292\n",
            "{'joy'}\n",
            "Layer: 2, Index: 389\n",
            "{'bryce'}\n",
            "Layer: 2, Index: 761\n",
            "{'mare'}\n",
            "Layer: 2, Index: 1046\n",
            "{'bro'}\n",
            "Layer: 2, Index: 2272\n",
            "{'gene'}\n",
            "Layer: 2, Index: 2536\n",
            "{'dan'}\n",
            "Layer: 2, Index: 2765\n",
            "{'bryce'}\n",
            "Layer: 2, Index: 2919\n",
            "{'uncle'}\n",
            "Layer: 2, Index: 4032\n",
            "{'ellen'}\n",
            "Scanning layer 3\n",
            "Layer: 3, Index: 158\n",
            "{'eva'}\n",
            "Layer: 3, Index: 329\n",
            "{'ada'}\n",
            "Layer: 3, Index: 954\n",
            "{'maxwell'}\n",
            "Layer: 3, Index: 1136\n",
            "{'ellen'}\n",
            "Layer: 3, Index: 1229\n",
            "{'faith'}\n",
            "Layer: 3, Index: 1653\n",
            "{'bryce'}\n",
            "Layer: 3, Index: 1674\n",
            "{'joy'}\n",
            "Layer: 3, Index: 1755\n",
            "{'leon'}\n",
            "Layer: 3, Index: 1769\n",
            "{'clinton'}\n",
            "Layer: 3, Index: 1879\n",
            "{'mary'}\n",
            "Layer: 3, Index: 1895\n",
            "{'jesus'}\n",
            "Layer: 3, Index: 2629\n",
            "{'alan'}\n",
            "Layer: 3, Index: 2645\n",
            "{'eva'}\n",
            "Layer: 3, Index: 3077\n",
            "{'ida'}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6a42f00858fa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# if(index % 1000):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#     print(f\"Scanning index {index}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_value_to_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgender_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Layer: {layer}, Index: {index}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/CS685/debiasing-ffn-updates/gpt2_wrapper.py\u001b[0m in \u001b[0;36mproject_value_to_vocab\u001b[0;34m(self, layer, value_idx, top_k)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mnormed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Search through layers\n",
        "layer_idx_dict = {}\n",
        "for layer in range(24):\n",
        "    print(f\"Scanning layer {layer}\")\n",
        "    for index in range(4096):\n",
        "        # if(index % 1000):\n",
        "        #     print(f\"Scanning index {index}\")\n",
        "        top_k = set([word.lower() for word, prob in wrapper.project_value_to_vocab(layer, index, 1)])\n",
        "        if(len(top_k.intersection(gender_terms)) >= 1):\n",
        "            print(f\"Layer: {layer}, Index: {index}\")\n",
        "            print(top_k)\n",
        "            key = str(layer)+\"_\"+str(index)\n",
        "            layer_idx_dict[key] = top_k\n",
        "        # if \"man\" in top_k or \"woman\" in top_k:\n",
        "        #     print(f\"Layer: {layer}, Index: {index}\")\n",
        "        #     print(top_k)"
      ],
      "id": "cc5dcb88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5977ca7d"
      },
      "outputs": [],
      "source": [],
      "id": "5977ca7d"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}