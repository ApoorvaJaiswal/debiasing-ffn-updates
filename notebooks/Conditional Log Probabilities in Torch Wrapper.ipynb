{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687234df",
   "metadata": {},
   "source": [
    "### Notebook Description ###\n",
    "Sandbox notebook to play around with some of the code we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d919c823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perspective_api import PerspectiveApiScorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7a0c1",
   "metadata": {},
   "source": [
    "### Perspective API Playground ###\n",
    "\n",
    "Please note this API is rate limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b165e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyDRX9GYuTijhdqk_JF20puTcCR0p2YwCA8\"\n",
    "\n",
    "scorer = PerspectiveApiScorer(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer.get_scores(\"I strongly dislike you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd4bb4",
   "metadata": {},
   "source": [
    "### Rewriting model.cond_log_probs() and model.score() ###\n",
    "\n",
    "Unfortunately, the wrapper classes used by their code use the PyTorch version of GPT-2 from hugging face.  The Big Bench tests all use Tensorflow version. \n",
    "\n",
    "The tasks we are interested in (BBQlite, Gender Sensitivity English, and Diverse Social Bias) rely on the cond_log_probs() and score() methods of this wrapper class.  I think we would just need to re-implement these in PyTorch\n",
    "\n",
    "Below are the wrapper classes for gpt2-medium from the Geva paper and from Big Bench. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f42cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model_wrappers.gpt2_wrapper import GPT2Wrapper\n",
    "pytorch_gpt2 = GPT2Wrapper(model_name = \"gpt2-medium\", use_cuda = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00dcdc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from bigbench.models.huggingface_models import _HFTransformerModel, BIGBenchHFModel\n",
    "bigbench_gpt2 = BIGBenchHFModel(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d224e",
   "metadata": {},
   "source": [
    "### Goal ###\n",
    "\n",
    "We want the output from bigbench_gpt2.cond_log_prob() and bigbench_gpt2.score() to match pytorch_gpt.cond_log_prob() and pytorch_gpt.score() respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96b0fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "prompt:\n",
      "What color is the sky? Answer: blue\n",
      "What color is grass? Answer:\n",
      "scores:\n",
      "       red: -1.29\n",
      "      blue: -2.09\n",
      "     green: -0.51\n"
     ]
    }
   ],
   "source": [
    "prompt = (f\"What color is the sky? Answer: blue\\n\" f\"What color is grass? Answer:\")\n",
    "choices =(\"red\", \"blue\", \"green\")\n",
    "\n",
    "scores = bigbench_gpt2.cond_log_prob(inputs=prompt, targets=choices)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"prompt:\\n{prompt}\")\n",
    "print(f\"scores:\")\n",
    "for c, s in zip(choices, scores):\n",
    "    print(f\"  {c:>8}: {s:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37aa02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inputs = ['What color is the sky? Answer: blue\\nWhat color is grass? Answer:', 'What color is the sky? Answer: blue\\nWhat color is grass? Answer:', 'What color is the sky? Answer: blue\\nWhat color is grass? Answer:']\n",
    "batch_targets = ['red', 'blue', 'green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dfce4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderpeterson/Documents/School/UMass/CS685/project/debiasing-ffn-updates/model_wrappers/gpt2_wrapper.py:361: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets: Union[List[str], List[List[str]]],\n",
      "/Users/alexanderpeterson/Documents/School/UMass/CS685/project/debiasing-ffn-updates/model_wrappers/gpt2_wrapper.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ) -> Union[List[float], List[List[float]]]:\n",
      "/Users/alexanderpeterson/Documents/School/UMass/CS685/project/debiasing-ffn-updates/model_wrappers/gpt2_wrapper.py:322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets:  targets to be scored\n"
     ]
    }
   ],
   "source": [
    "pytorch_loss = pytorch_gpt2.score(inputs=batch_inputs, targets=batch_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c93ce71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigbench_loss = bigbench_gpt2._model.score(inputs=batch_inputs, targets=batch_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343ea49",
   "metadata": {},
   "source": [
    "It looks like the logits each model produces are off be a precision point... Thoughts on if this is ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7459a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.42503547668457, -11.222871780395508, -9.64091682434082]\n",
      "[-10.4252290725708, -11.22307300567627, -9.641101837158203]\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_loss)\n",
    "print(bigbench_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4acb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderpeterson/Documents/School/UMass/CS685/project/debiasing-ffn-updates/model_wrappers/gpt2_wrapper.py:361: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_ids = torch.tensor(tokenized_ids[\"targets_ids\"])\n",
      "/Users/alexanderpeterson/Documents/School/UMass/CS685/project/debiasing-ffn-updates/model_wrappers/gpt2_wrapper.py:364: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  position_ids = torch.maximum(torch.cumsum(torch.tensor(attention_mask), axis=-1) - 1, torch.tensor(0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "prompt:\n",
      "What color is the sky? Answer: blue\n",
      "What color is grass? Answer:\n",
      "scores:\n",
      "       red: -1.29\n",
      "      blue: -2.09\n",
      "     green: -0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderpeterson/Documents/School/UMass/CS685/project/debiasing-ffn-updates/model_wrappers/gpt2_wrapper.py:322: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  logits = torch.tensor(logits, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "pytorch_scores = pytorch_gpt2.cond_log_prob(inputs=prompt, targets=choices)\n",
    "print(\"\\n\")\n",
    "print(f\"prompt:\\n{prompt}\")\n",
    "print(f\"scores:\")\n",
    "for c, s in zip(choices, pytorch_scores):\n",
    "    print(f\"  {c:>8}: {s:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a91c62",
   "metadata": {},
   "source": [
    "Now lets try passing in several at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfbb8c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "prompt:\n",
      "What color is the sky? Answer: blue\n",
      "What color is grass? Answer: \n",
      "scores:\n",
      "       red: -0.14\n",
      "      blue: -2.36\n",
      "     green: -3.41\n",
      "\n",
      "\n",
      "prompt:\n",
      "What is 1+1? Answer: 2\n",
      "What is 2+2? Answer: \n",
      "scores:\n",
      "         1: -1.48\n",
      "         2: -0.96\n",
      "         3: -1.01\n",
      "         4: -3.79\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    f\"What color is the sky? Answer: blue\\n\" f\"What color is grass? Answer: \",\n",
    "    f\"What is 1+1? Answer: 2\\n\" f\"What is 2+2? Answer: \",\n",
    "]\n",
    "choices = [(\"red\", \"blue\", \"green\"), (\"1\", \"2\", \"3\", \"4\")]\n",
    "\n",
    "scores = pytorch_gpt2.cond_log_prob(inputs=prompts, targets=choices)\n",
    "\n",
    "for p, c, s in zip(prompts, choices, scores):\n",
    "    print(\"\\n\")\n",
    "    print(f\"prompt:\\n{p}\")\n",
    "    print(f\"scores:\")\n",
    "    for ci, si in zip(c, s):\n",
    "        print(f\"  {ci:>8}: {si:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe7473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
